{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20109/2056488116.py:54: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  their_params = {key: jax.numpy.array(numpy.array(value.detach())) for key, value in transformer.named_parameters()}\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import torch\n",
    "import numpy\n",
    "import jax\n",
    "\n",
    "from moshi.moshi.modules.transformer import ProjectedTransformer\n",
    "from moshi_jax.moshi_jax.modules.transformer import ProjectedTransformer as JAXTransformer\n",
    "\n",
    "\n",
    "_seanet_kwargs = {\n",
    "    \"channels\": 1,\n",
    "    \"dimension\": 512,\n",
    "    \"causal\": True,\n",
    "    \"n_filters\": 64,\n",
    "    \"n_residual_layers\": 1,\n",
    "    \"activation\": \"ELU\",\n",
    "    \"compress\": 2,\n",
    "    \"dilation_base\": 2,\n",
    "    \"disable_norm_outer_blocks\": 0,\n",
    "    \"kernel_size\": 7,\n",
    "    \"residual_kernel_size\": 3,\n",
    "    \"last_kernel_size\": 3,\n",
    "    # We train using weight_norm but then the weights are pre-processed for inference so\n",
    "    # that we can use a normal convolution.\n",
    "    \"norm\": \"none\",\n",
    "    \"pad_mode\": \"constant\",\n",
    "    \"ratios\": [8, 6, 5, 4],\n",
    "    \"true_skip\": True,\n",
    "}\n",
    "\n",
    "_transformer_kwargs = {\n",
    "    \"d_model\": _seanet_kwargs[\"dimension\"],\n",
    "    \"num_heads\": 8,\n",
    "    \"num_layers\": 8,\n",
    "    \"causal\": True,\n",
    "    \"layer_scale\": 0.01,\n",
    "    \"context\": 250,\n",
    "    \"conv_layout\": True,\n",
    "    \"max_period\": 10000,\n",
    "    \"gating\": \"none\",\n",
    "    \"norm\": \"layer_norm\",\n",
    "    \"positional_embedding\": \"rope\",\n",
    "    \"dim_feedforward\": 2048,\n",
    "    \"input_dimension\": _seanet_kwargs[\"dimension\"],\n",
    "    \"output_dimensions\": [_seanet_kwargs[\"dimension\"]],\n",
    "}\n",
    "\n",
    "device = torch.get_default_device()\n",
    "\n",
    "transformer = ProjectedTransformer(\n",
    "        device=device, **_transformer_kwargs\n",
    "    )\n",
    "jax_transformer = JAXTransformer(**_transformer_kwargs, key=jax.random.key(1))\n",
    "their_params = {key: jax.numpy.array(numpy.array(value.detach())) for key, value in transformer.named_parameters()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['transformer.layers.0.self_attn.in_proj_weight', 'transformer.layers.0.self_attn.out_proj.weight', 'transformer.layers.0.norm1.weight', 'transformer.layers.0.norm1.bias', 'transformer.layers.0.norm2.weight', 'transformer.layers.0.norm2.bias', 'transformer.layers.0.linear1.weight', 'transformer.layers.0.linear2.weight', 'transformer.layers.0.layer_scale_1.scale', 'transformer.layers.0.layer_scale_2.scale', 'transformer.layers.1.self_attn.in_proj_weight', 'transformer.layers.1.self_attn.out_proj.weight', 'transformer.layers.1.norm1.weight', 'transformer.layers.1.norm1.bias', 'transformer.layers.1.norm2.weight', 'transformer.layers.1.norm2.bias', 'transformer.layers.1.linear1.weight', 'transformer.layers.1.linear2.weight', 'transformer.layers.1.layer_scale_1.scale', 'transformer.layers.1.layer_scale_2.scale', 'transformer.layers.2.self_attn.in_proj_weight', 'transformer.layers.2.self_attn.out_proj.weight', 'transformer.layers.2.norm1.weight', 'transformer.layers.2.norm1.bias', 'transformer.layers.2.norm2.weight', 'transformer.layers.2.norm2.bias', 'transformer.layers.2.linear1.weight', 'transformer.layers.2.linear2.weight', 'transformer.layers.2.layer_scale_1.scale', 'transformer.layers.2.layer_scale_2.scale', 'transformer.layers.3.self_attn.in_proj_weight', 'transformer.layers.3.self_attn.out_proj.weight', 'transformer.layers.3.norm1.weight', 'transformer.layers.3.norm1.bias', 'transformer.layers.3.norm2.weight', 'transformer.layers.3.norm2.bias', 'transformer.layers.3.linear1.weight', 'transformer.layers.3.linear2.weight', 'transformer.layers.3.layer_scale_1.scale', 'transformer.layers.3.layer_scale_2.scale', 'transformer.layers.4.self_attn.in_proj_weight', 'transformer.layers.4.self_attn.out_proj.weight', 'transformer.layers.4.norm1.weight', 'transformer.layers.4.norm1.bias', 'transformer.layers.4.norm2.weight', 'transformer.layers.4.norm2.bias', 'transformer.layers.4.linear1.weight', 'transformer.layers.4.linear2.weight', 'transformer.layers.4.layer_scale_1.scale', 'transformer.layers.4.layer_scale_2.scale', 'transformer.layers.5.self_attn.in_proj_weight', 'transformer.layers.5.self_attn.out_proj.weight', 'transformer.layers.5.norm1.weight', 'transformer.layers.5.norm1.bias', 'transformer.layers.5.norm2.weight', 'transformer.layers.5.norm2.bias', 'transformer.layers.5.linear1.weight', 'transformer.layers.5.linear2.weight', 'transformer.layers.5.layer_scale_1.scale', 'transformer.layers.5.layer_scale_2.scale', 'transformer.layers.6.self_attn.in_proj_weight', 'transformer.layers.6.self_attn.out_proj.weight', 'transformer.layers.6.norm1.weight', 'transformer.layers.6.norm1.bias', 'transformer.layers.6.norm2.weight', 'transformer.layers.6.norm2.bias', 'transformer.layers.6.linear1.weight', 'transformer.layers.6.linear2.weight', 'transformer.layers.6.layer_scale_1.scale', 'transformer.layers.6.layer_scale_2.scale', 'transformer.layers.7.self_attn.in_proj_weight', 'transformer.layers.7.self_attn.out_proj.weight', 'transformer.layers.7.norm1.weight', 'transformer.layers.7.norm1.bias', 'transformer.layers.7.norm2.weight', 'transformer.layers.7.norm2.bias', 'transformer.layers.7.linear1.weight', 'transformer.layers.7.linear2.weight', 'transformer.layers.7.layer_scale_1.scale', 'transformer.layers.7.layer_scale_2.scale'])\n",
      "ModuleList(\n",
      "  (0): Identity()\n",
      ")\n",
      "[Identity()]\n"
     ]
    }
   ],
   "source": [
    "print(their_params.keys())\n",
    "print(transformer.output_projs)\n",
    "print(jax_transformer.output_projs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".transformer.positional_embedding\n",
      ".transformer.max_period\n",
      ".transformer.positional_scale\n",
      ".transformer.rope.max_period\n",
      ".transformer.layers[0].linear1.weight\n",
      ".transformer.layers[0].linear2.weight\n",
      ".transformer.layers[0].self_attn.embed_dim\n",
      ".transformer.layers[0].self_attn.causal\n",
      ".transformer.layers[0].self_attn.context\n",
      ".transformer.layers[0].self_attn.rope.max_period\n",
      ".transformer.layers[0].self_attn.num_heads\n",
      ".transformer.layers[0].self_attn.weights_per_step\n",
      ".transformer.layers[0].self_attn.in_proj.weight\n",
      ".transformer.layers[0].self_attn.out_proj.weight\n",
      ".transformer.layers[0].norm1.weight\n",
      ".transformer.layers[0].norm1.bias\n",
      ".transformer.layers[0].norm2.weight\n",
      ".transformer.layers[0].norm2.bias\n",
      ".transformer.layers[0].skip_self_attn\n",
      ".transformer.layers[0].activation\n",
      ".transformer.layers[0].layer_scale_1.scale\n",
      ".transformer.layers[0].layer_scale_1.channel_last\n",
      ".transformer.layers[0].layer_scale_2.scale\n",
      ".transformer.layers[0].layer_scale_2.channel_last\n",
      ".transformer.layers[0].weights_per_step\n",
      ".transformer.layers[1].linear1.weight\n",
      ".transformer.layers[1].linear2.weight\n",
      ".transformer.layers[1].self_attn.embed_dim\n",
      ".transformer.layers[1].self_attn.causal\n",
      ".transformer.layers[1].self_attn.context\n",
      ".transformer.layers[1].self_attn.rope.max_period\n",
      ".transformer.layers[1].self_attn.num_heads\n",
      ".transformer.layers[1].self_attn.weights_per_step\n",
      ".transformer.layers[1].self_attn.in_proj.weight\n",
      ".transformer.layers[1].self_attn.out_proj.weight\n",
      ".transformer.layers[1].norm1.weight\n",
      ".transformer.layers[1].norm1.bias\n",
      ".transformer.layers[1].norm2.weight\n",
      ".transformer.layers[1].norm2.bias\n",
      ".transformer.layers[1].skip_self_attn\n",
      ".transformer.layers[1].activation\n",
      ".transformer.layers[1].layer_scale_1.scale\n",
      ".transformer.layers[1].layer_scale_1.channel_last\n",
      ".transformer.layers[1].layer_scale_2.scale\n",
      ".transformer.layers[1].layer_scale_2.channel_last\n",
      ".transformer.layers[1].weights_per_step\n",
      ".transformer.layers[2].linear1.weight\n",
      ".transformer.layers[2].linear2.weight\n",
      ".transformer.layers[2].self_attn.embed_dim\n",
      ".transformer.layers[2].self_attn.causal\n",
      ".transformer.layers[2].self_attn.context\n",
      ".transformer.layers[2].self_attn.rope.max_period\n",
      ".transformer.layers[2].self_attn.num_heads\n",
      ".transformer.layers[2].self_attn.weights_per_step\n",
      ".transformer.layers[2].self_attn.in_proj.weight\n",
      ".transformer.layers[2].self_attn.out_proj.weight\n",
      ".transformer.layers[2].norm1.weight\n",
      ".transformer.layers[2].norm1.bias\n",
      ".transformer.layers[2].norm2.weight\n",
      ".transformer.layers[2].norm2.bias\n",
      ".transformer.layers[2].skip_self_attn\n",
      ".transformer.layers[2].activation\n",
      ".transformer.layers[2].layer_scale_1.scale\n",
      ".transformer.layers[2].layer_scale_1.channel_last\n",
      ".transformer.layers[2].layer_scale_2.scale\n",
      ".transformer.layers[2].layer_scale_2.channel_last\n",
      ".transformer.layers[2].weights_per_step\n",
      ".transformer.layers[3].linear1.weight\n",
      ".transformer.layers[3].linear2.weight\n",
      ".transformer.layers[3].self_attn.embed_dim\n",
      ".transformer.layers[3].self_attn.causal\n",
      ".transformer.layers[3].self_attn.context\n",
      ".transformer.layers[3].self_attn.rope.max_period\n",
      ".transformer.layers[3].self_attn.num_heads\n",
      ".transformer.layers[3].self_attn.weights_per_step\n",
      ".transformer.layers[3].self_attn.in_proj.weight\n",
      ".transformer.layers[3].self_attn.out_proj.weight\n",
      ".transformer.layers[3].norm1.weight\n",
      ".transformer.layers[3].norm1.bias\n",
      ".transformer.layers[3].norm2.weight\n",
      ".transformer.layers[3].norm2.bias\n",
      ".transformer.layers[3].skip_self_attn\n",
      ".transformer.layers[3].activation\n",
      ".transformer.layers[3].layer_scale_1.scale\n",
      ".transformer.layers[3].layer_scale_1.channel_last\n",
      ".transformer.layers[3].layer_scale_2.scale\n",
      ".transformer.layers[3].layer_scale_2.channel_last\n",
      ".transformer.layers[3].weights_per_step\n",
      ".transformer.layers[4].linear1.weight\n",
      ".transformer.layers[4].linear2.weight\n",
      ".transformer.layers[4].self_attn.embed_dim\n",
      ".transformer.layers[4].self_attn.causal\n",
      ".transformer.layers[4].self_attn.context\n",
      ".transformer.layers[4].self_attn.rope.max_period\n",
      ".transformer.layers[4].self_attn.num_heads\n",
      ".transformer.layers[4].self_attn.weights_per_step\n",
      ".transformer.layers[4].self_attn.in_proj.weight\n",
      ".transformer.layers[4].self_attn.out_proj.weight\n",
      ".transformer.layers[4].norm1.weight\n",
      ".transformer.layers[4].norm1.bias\n",
      ".transformer.layers[4].norm2.weight\n",
      ".transformer.layers[4].norm2.bias\n",
      ".transformer.layers[4].skip_self_attn\n",
      ".transformer.layers[4].activation\n",
      ".transformer.layers[4].layer_scale_1.scale\n",
      ".transformer.layers[4].layer_scale_1.channel_last\n",
      ".transformer.layers[4].layer_scale_2.scale\n",
      ".transformer.layers[4].layer_scale_2.channel_last\n",
      ".transformer.layers[4].weights_per_step\n",
      ".transformer.layers[5].linear1.weight\n",
      ".transformer.layers[5].linear2.weight\n",
      ".transformer.layers[5].self_attn.embed_dim\n",
      ".transformer.layers[5].self_attn.causal\n",
      ".transformer.layers[5].self_attn.context\n",
      ".transformer.layers[5].self_attn.rope.max_period\n",
      ".transformer.layers[5].self_attn.num_heads\n",
      ".transformer.layers[5].self_attn.weights_per_step\n",
      ".transformer.layers[5].self_attn.in_proj.weight\n",
      ".transformer.layers[5].self_attn.out_proj.weight\n",
      ".transformer.layers[5].norm1.weight\n",
      ".transformer.layers[5].norm1.bias\n",
      ".transformer.layers[5].norm2.weight\n",
      ".transformer.layers[5].norm2.bias\n",
      ".transformer.layers[5].skip_self_attn\n",
      ".transformer.layers[5].activation\n",
      ".transformer.layers[5].layer_scale_1.scale\n",
      ".transformer.layers[5].layer_scale_1.channel_last\n",
      ".transformer.layers[5].layer_scale_2.scale\n",
      ".transformer.layers[5].layer_scale_2.channel_last\n",
      ".transformer.layers[5].weights_per_step\n",
      ".transformer.layers[6].linear1.weight\n",
      ".transformer.layers[6].linear2.weight\n",
      ".transformer.layers[6].self_attn.embed_dim\n",
      ".transformer.layers[6].self_attn.causal\n",
      ".transformer.layers[6].self_attn.context\n",
      ".transformer.layers[6].self_attn.rope.max_period\n",
      ".transformer.layers[6].self_attn.num_heads\n",
      ".transformer.layers[6].self_attn.weights_per_step\n",
      ".transformer.layers[6].self_attn.in_proj.weight\n",
      ".transformer.layers[6].self_attn.out_proj.weight\n",
      ".transformer.layers[6].norm1.weight\n",
      ".transformer.layers[6].norm1.bias\n",
      ".transformer.layers[6].norm2.weight\n",
      ".transformer.layers[6].norm2.bias\n",
      ".transformer.layers[6].skip_self_attn\n",
      ".transformer.layers[6].activation\n",
      ".transformer.layers[6].layer_scale_1.scale\n",
      ".transformer.layers[6].layer_scale_1.channel_last\n",
      ".transformer.layers[6].layer_scale_2.scale\n",
      ".transformer.layers[6].layer_scale_2.channel_last\n",
      ".transformer.layers[6].weights_per_step\n",
      ".transformer.layers[7].linear1.weight\n",
      ".transformer.layers[7].linear2.weight\n",
      ".transformer.layers[7].self_attn.embed_dim\n",
      ".transformer.layers[7].self_attn.causal\n",
      ".transformer.layers[7].self_attn.context\n",
      ".transformer.layers[7].self_attn.rope.max_period\n",
      ".transformer.layers[7].self_attn.num_heads\n",
      ".transformer.layers[7].self_attn.weights_per_step\n",
      ".transformer.layers[7].self_attn.in_proj.weight\n",
      ".transformer.layers[7].self_attn.out_proj.weight\n",
      ".transformer.layers[7].norm1.weight\n",
      ".transformer.layers[7].norm1.bias\n",
      ".transformer.layers[7].norm2.weight\n",
      ".transformer.layers[7].norm2.bias\n",
      ".transformer.layers[7].skip_self_attn\n",
      ".transformer.layers[7].activation\n",
      ".transformer.layers[7].layer_scale_1.scale\n",
      ".transformer.layers[7].layer_scale_1.channel_last\n",
      ".transformer.layers[7].layer_scale_2.scale\n",
      ".transformer.layers[7].layer_scale_2.channel_last\n",
      ".transformer.layers[7].weights_per_step\n",
      ".input_dimension\n",
      ".output_dimensions[0]\n",
      ".conv_layout\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ProjectedTransformer(\n",
       "  transformer=StreamingTransformer(\n",
       "    positional_embedding=None,\n",
       "    max_period=None,\n",
       "    positional_scale=None,\n",
       "    betas=None,\n",
       "    rope=RotaryEmbedding(max_period=None),\n",
       "    layers=[\n",
       "      StreamingTransformerLayer(\n",
       "        gating=None,\n",
       "        linear1=Linear(\n",
       "          weight=None,\n",
       "          bias=None,\n",
       "          in_features=512,\n",
       "          out_features=2048,\n",
       "          use_bias=False\n",
       "        ),\n",
       "        linear2=Linear(\n",
       "          weight=None,\n",
       "          bias=None,\n",
       "          in_features=2048,\n",
       "          out_features=512,\n",
       "          use_bias=False\n",
       "        ),\n",
       "        self_attn=StreamingMultiheadAttention(\n",
       "          embed_dim=None,\n",
       "          causal=None,\n",
       "          context=None,\n",
       "          rope=RotaryEmbedding(max_period=None),\n",
       "          num_heads=None,\n",
       "          weights_per_step=None,\n",
       "          in_proj=Linear(\n",
       "            weight=None,\n",
       "            bias=None,\n",
       "            in_features=512,\n",
       "            out_features=1536,\n",
       "            use_bias=False\n",
       "          ),\n",
       "          out_proj=Linear(\n",
       "            weight=None,\n",
       "            bias=None,\n",
       "            in_features=512,\n",
       "            out_features=512,\n",
       "            use_bias=False\n",
       "          )\n",
       "        ),\n",
       "        norm1=LayerNorm(\n",
       "          shape=(512,),\n",
       "          eps=1e-05,\n",
       "          use_weight=True,\n",
       "          use_bias=True,\n",
       "          weight=None,\n",
       "          bias=None\n",
       "        ),\n",
       "        norm2=LayerNorm(\n",
       "          shape=(512,),\n",
       "          eps=1e-05,\n",
       "          use_weight=True,\n",
       "          use_bias=True,\n",
       "          weight=None,\n",
       "          bias=None\n",
       "        ),\n",
       "        skip_self_attn=None,\n",
       "        activation=None,\n",
       "        layer_scale_1=LayerScale(scale=None, channel_last=None),\n",
       "        layer_scale_2=LayerScale(scale=None, channel_last=None),\n",
       "        weights_per_step=None\n",
       "      ),\n",
       "      StreamingTransformerLayer(\n",
       "        gating=None,\n",
       "        linear1=Linear(\n",
       "          weight=None,\n",
       "          bias=None,\n",
       "          in_features=512,\n",
       "          out_features=2048,\n",
       "          use_bias=False\n",
       "        ),\n",
       "        linear2=Linear(\n",
       "          weight=None,\n",
       "          bias=None,\n",
       "          in_features=2048,\n",
       "          out_features=512,\n",
       "          use_bias=False\n",
       "        ),\n",
       "        self_attn=StreamingMultiheadAttention(\n",
       "          embed_dim=None,\n",
       "          causal=None,\n",
       "          context=None,\n",
       "          rope=RotaryEmbedding(max_period=None),\n",
       "          num_heads=None,\n",
       "          weights_per_step=None,\n",
       "          in_proj=Linear(\n",
       "            weight=None,\n",
       "            bias=None,\n",
       "            in_features=512,\n",
       "            out_features=1536,\n",
       "            use_bias=False\n",
       "          ),\n",
       "          out_proj=Linear(\n",
       "            weight=None,\n",
       "            bias=None,\n",
       "            in_features=512,\n",
       "            out_features=512,\n",
       "            use_bias=False\n",
       "          )\n",
       "        ),\n",
       "        norm1=LayerNorm(\n",
       "          shape=(512,),\n",
       "          eps=1e-05,\n",
       "          use_weight=True,\n",
       "          use_bias=True,\n",
       "          weight=None,\n",
       "          bias=None\n",
       "        ),\n",
       "        norm2=LayerNorm(\n",
       "          shape=(512,),\n",
       "          eps=1e-05,\n",
       "          use_weight=True,\n",
       "          use_bias=True,\n",
       "          weight=None,\n",
       "          bias=None\n",
       "        ),\n",
       "        skip_self_attn=None,\n",
       "        activation=None,\n",
       "        layer_scale_1=LayerScale(scale=None, channel_last=None),\n",
       "        layer_scale_2=LayerScale(scale=None, channel_last=None),\n",
       "        weights_per_step=None\n",
       "      ),\n",
       "      StreamingTransformerLayer(\n",
       "        gating=None,\n",
       "        linear1=Linear(\n",
       "          weight=None,\n",
       "          bias=None,\n",
       "          in_features=512,\n",
       "          out_features=2048,\n",
       "          use_bias=False\n",
       "        ),\n",
       "        linear2=Linear(\n",
       "          weight=None,\n",
       "          bias=None,\n",
       "          in_features=2048,\n",
       "          out_features=512,\n",
       "          use_bias=False\n",
       "        ),\n",
       "        self_attn=StreamingMultiheadAttention(\n",
       "          embed_dim=None,\n",
       "          causal=None,\n",
       "          context=None,\n",
       "          rope=RotaryEmbedding(max_period=None),\n",
       "          num_heads=None,\n",
       "          weights_per_step=None,\n",
       "          in_proj=Linear(\n",
       "            weight=None,\n",
       "            bias=None,\n",
       "            in_features=512,\n",
       "            out_features=1536,\n",
       "            use_bias=False\n",
       "          ),\n",
       "          out_proj=Linear(\n",
       "            weight=None,\n",
       "            bias=None,\n",
       "            in_features=512,\n",
       "            out_features=512,\n",
       "            use_bias=False\n",
       "          )\n",
       "        ),\n",
       "        norm1=LayerNorm(\n",
       "          shape=(512,),\n",
       "          eps=1e-05,\n",
       "          use_weight=True,\n",
       "          use_bias=True,\n",
       "          weight=None,\n",
       "          bias=None\n",
       "        ),\n",
       "        norm2=LayerNorm(\n",
       "          shape=(512,),\n",
       "          eps=1e-05,\n",
       "          use_weight=True,\n",
       "          use_bias=True,\n",
       "          weight=None,\n",
       "          bias=None\n",
       "        ),\n",
       "        skip_self_attn=None,\n",
       "        activation=None,\n",
       "        layer_scale_1=LayerScale(scale=None, channel_last=None),\n",
       "        layer_scale_2=LayerScale(scale=None, channel_last=None),\n",
       "        weights_per_step=None\n",
       "      ),\n",
       "      StreamingTransformerLayer(\n",
       "        gating=None,\n",
       "        linear1=Linear(\n",
       "          weight=None,\n",
       "          bias=None,\n",
       "          in_features=512,\n",
       "          out_features=2048,\n",
       "          use_bias=False\n",
       "        ),\n",
       "        linear2=Linear(\n",
       "          weight=None,\n",
       "          bias=None,\n",
       "          in_features=2048,\n",
       "          out_features=512,\n",
       "          use_bias=False\n",
       "        ),\n",
       "        self_attn=StreamingMultiheadAttention(\n",
       "          embed_dim=None,\n",
       "          causal=None,\n",
       "          context=None,\n",
       "          rope=RotaryEmbedding(max_period=None),\n",
       "          num_heads=None,\n",
       "          weights_per_step=None,\n",
       "          in_proj=Linear(\n",
       "            weight=None,\n",
       "            bias=None,\n",
       "            in_features=512,\n",
       "            out_features=1536,\n",
       "            use_bias=False\n",
       "          ),\n",
       "          out_proj=Linear(\n",
       "            weight=None,\n",
       "            bias=None,\n",
       "            in_features=512,\n",
       "            out_features=512,\n",
       "            use_bias=False\n",
       "          )\n",
       "        ),\n",
       "        norm1=LayerNorm(\n",
       "          shape=(512,),\n",
       "          eps=1e-05,\n",
       "          use_weight=True,\n",
       "          use_bias=True,\n",
       "          weight=None,\n",
       "          bias=None\n",
       "        ),\n",
       "        norm2=LayerNorm(\n",
       "          shape=(512,),\n",
       "          eps=1e-05,\n",
       "          use_weight=True,\n",
       "          use_bias=True,\n",
       "          weight=None,\n",
       "          bias=None\n",
       "        ),\n",
       "        skip_self_attn=None,\n",
       "        activation=None,\n",
       "        layer_scale_1=LayerScale(scale=None, channel_last=None),\n",
       "        layer_scale_2=LayerScale(scale=None, channel_last=None),\n",
       "        weights_per_step=None\n",
       "      ),\n",
       "      StreamingTransformerLayer(\n",
       "        gating=None,\n",
       "        linear1=Linear(\n",
       "          weight=None,\n",
       "          bias=None,\n",
       "          in_features=512,\n",
       "          out_features=2048,\n",
       "          use_bias=False\n",
       "        ),\n",
       "        linear2=Linear(\n",
       "          weight=None,\n",
       "          bias=None,\n",
       "          in_features=2048,\n",
       "          out_features=512,\n",
       "          use_bias=False\n",
       "        ),\n",
       "        self_attn=StreamingMultiheadAttention(\n",
       "          embed_dim=None,\n",
       "          causal=None,\n",
       "          context=None,\n",
       "          rope=RotaryEmbedding(max_period=None),\n",
       "          num_heads=None,\n",
       "          weights_per_step=None,\n",
       "          in_proj=Linear(\n",
       "            weight=None,\n",
       "            bias=None,\n",
       "            in_features=512,\n",
       "            out_features=1536,\n",
       "            use_bias=False\n",
       "          ),\n",
       "          out_proj=Linear(\n",
       "            weight=None,\n",
       "            bias=None,\n",
       "            in_features=512,\n",
       "            out_features=512,\n",
       "            use_bias=False\n",
       "          )\n",
       "        ),\n",
       "        norm1=LayerNorm(\n",
       "          shape=(512,),\n",
       "          eps=1e-05,\n",
       "          use_weight=True,\n",
       "          use_bias=True,\n",
       "          weight=None,\n",
       "          bias=None\n",
       "        ),\n",
       "        norm2=LayerNorm(\n",
       "          shape=(512,),\n",
       "          eps=1e-05,\n",
       "          use_weight=True,\n",
       "          use_bias=True,\n",
       "          weight=None,\n",
       "          bias=None\n",
       "        ),\n",
       "        skip_self_attn=None,\n",
       "        activation=None,\n",
       "        layer_scale_1=LayerScale(scale=None, channel_last=None),\n",
       "        layer_scale_2=LayerScale(scale=None, channel_last=None),\n",
       "        weights_per_step=None\n",
       "      ),\n",
       "      StreamingTransformerLayer(\n",
       "        gating=None,\n",
       "        linear1=Linear(\n",
       "          weight=None,\n",
       "          bias=None,\n",
       "          in_features=512,\n",
       "          out_features=2048,\n",
       "          use_bias=False\n",
       "        ),\n",
       "        linear2=Linear(\n",
       "          weight=None,\n",
       "          bias=None,\n",
       "          in_features=2048,\n",
       "          out_features=512,\n",
       "          use_bias=False\n",
       "        ),\n",
       "        self_attn=StreamingMultiheadAttention(\n",
       "          embed_dim=None,\n",
       "          causal=None,\n",
       "          context=None,\n",
       "          rope=RotaryEmbedding(max_period=None),\n",
       "          num_heads=None,\n",
       "          weights_per_step=None,\n",
       "          in_proj=Linear(\n",
       "            weight=None,\n",
       "            bias=None,\n",
       "            in_features=512,\n",
       "            out_features=1536,\n",
       "            use_bias=False\n",
       "          ),\n",
       "          out_proj=Linear(\n",
       "            weight=None,\n",
       "            bias=None,\n",
       "            in_features=512,\n",
       "            out_features=512,\n",
       "            use_bias=False\n",
       "          )\n",
       "        ),\n",
       "        norm1=LayerNorm(\n",
       "          shape=(512,),\n",
       "          eps=1e-05,\n",
       "          use_weight=True,\n",
       "          use_bias=True,\n",
       "          weight=None,\n",
       "          bias=None\n",
       "        ),\n",
       "        norm2=LayerNorm(\n",
       "          shape=(512,),\n",
       "          eps=1e-05,\n",
       "          use_weight=True,\n",
       "          use_bias=True,\n",
       "          weight=None,\n",
       "          bias=None\n",
       "        ),\n",
       "        skip_self_attn=None,\n",
       "        activation=None,\n",
       "        layer_scale_1=LayerScale(scale=None, channel_last=None),\n",
       "        layer_scale_2=LayerScale(scale=None, channel_last=None),\n",
       "        weights_per_step=None\n",
       "      ),\n",
       "      StreamingTransformerLayer(\n",
       "        gating=None,\n",
       "        linear1=Linear(\n",
       "          weight=None,\n",
       "          bias=None,\n",
       "          in_features=512,\n",
       "          out_features=2048,\n",
       "          use_bias=False\n",
       "        ),\n",
       "        linear2=Linear(\n",
       "          weight=None,\n",
       "          bias=None,\n",
       "          in_features=2048,\n",
       "          out_features=512,\n",
       "          use_bias=False\n",
       "        ),\n",
       "        self_attn=StreamingMultiheadAttention(\n",
       "          embed_dim=None,\n",
       "          causal=None,\n",
       "          context=None,\n",
       "          rope=RotaryEmbedding(max_period=None),\n",
       "          num_heads=None,\n",
       "          weights_per_step=None,\n",
       "          in_proj=Linear(\n",
       "            weight=None,\n",
       "            bias=None,\n",
       "            in_features=512,\n",
       "            out_features=1536,\n",
       "            use_bias=False\n",
       "          ),\n",
       "          out_proj=Linear(\n",
       "            weight=None,\n",
       "            bias=None,\n",
       "            in_features=512,\n",
       "            out_features=512,\n",
       "            use_bias=False\n",
       "          )\n",
       "        ),\n",
       "        norm1=LayerNorm(\n",
       "          shape=(512,),\n",
       "          eps=1e-05,\n",
       "          use_weight=True,\n",
       "          use_bias=True,\n",
       "          weight=None,\n",
       "          bias=None\n",
       "        ),\n",
       "        norm2=LayerNorm(\n",
       "          shape=(512,),\n",
       "          eps=1e-05,\n",
       "          use_weight=True,\n",
       "          use_bias=True,\n",
       "          weight=None,\n",
       "          bias=None\n",
       "        ),\n",
       "        skip_self_attn=None,\n",
       "        activation=None,\n",
       "        layer_scale_1=LayerScale(scale=None, channel_last=None),\n",
       "        layer_scale_2=LayerScale(scale=None, channel_last=None),\n",
       "        weights_per_step=None\n",
       "      ),\n",
       "      StreamingTransformerLayer(\n",
       "        gating=None,\n",
       "        linear1=Linear(\n",
       "          weight=None,\n",
       "          bias=None,\n",
       "          in_features=512,\n",
       "          out_features=2048,\n",
       "          use_bias=False\n",
       "        ),\n",
       "        linear2=Linear(\n",
       "          weight=None,\n",
       "          bias=None,\n",
       "          in_features=2048,\n",
       "          out_features=512,\n",
       "          use_bias=False\n",
       "        ),\n",
       "        self_attn=StreamingMultiheadAttention(\n",
       "          embed_dim=None,\n",
       "          causal=None,\n",
       "          context=None,\n",
       "          rope=RotaryEmbedding(max_period=None),\n",
       "          num_heads=None,\n",
       "          weights_per_step=None,\n",
       "          in_proj=Linear(\n",
       "            weight=None,\n",
       "            bias=None,\n",
       "            in_features=512,\n",
       "            out_features=1536,\n",
       "            use_bias=False\n",
       "          ),\n",
       "          out_proj=Linear(\n",
       "            weight=None,\n",
       "            bias=None,\n",
       "            in_features=512,\n",
       "            out_features=512,\n",
       "            use_bias=False\n",
       "          )\n",
       "        ),\n",
       "        norm1=LayerNorm(\n",
       "          shape=(512,),\n",
       "          eps=1e-05,\n",
       "          use_weight=True,\n",
       "          use_bias=True,\n",
       "          weight=None,\n",
       "          bias=None\n",
       "        ),\n",
       "        norm2=LayerNorm(\n",
       "          shape=(512,),\n",
       "          eps=1e-05,\n",
       "          use_weight=True,\n",
       "          use_bias=True,\n",
       "          weight=None,\n",
       "          bias=None\n",
       "        ),\n",
       "        skip_self_attn=None,\n",
       "        activation=None,\n",
       "        layer_scale_1=LayerScale(scale=None, channel_last=None),\n",
       "        layer_scale_2=LayerScale(scale=None, channel_last=None),\n",
       "        weights_per_step=None\n",
       "      )\n",
       "    ]\n",
       "  ),\n",
       "  input_dimension=None,\n",
       "  output_dimensions=[None],\n",
       "  output_projs=[Identity()],\n",
       "  input_proj=Identity(),\n",
       "  conv_layout=None\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.tree_util.tree_map_with_path(lambda p, x: print(jax.tree_util.keystr(p)), jax_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.layers[0].linear1.weight (2048, 512) (2048, 512)\n",
      "transformer.layers[0].linear2.weight (512, 2048) (512, 2048)\n",
      "transformer.layers[0].self_attn.in_proj.weight (1536, 512) (1536, 512)\n",
      "transformer.layers[0].self_attn.out_proj.weight (512, 512) (512, 512)\n",
      "transformer.layers[0].norm1.weight (512,) (512,)\n",
      "transformer.layers[0].norm1.bias (512,) (512,)\n",
      "transformer.layers[0].norm2.weight (512,) (512,)\n",
      "transformer.layers[0].norm2.bias (512,) (512,)\n",
      "transformer.layers[1].linear1.weight (2048, 512) (2048, 512)\n",
      "transformer.layers[1].linear2.weight (512, 2048) (512, 2048)\n",
      "transformer.layers[1].self_attn.in_proj.weight (1536, 512) (1536, 512)\n",
      "transformer.layers[1].self_attn.out_proj.weight (512, 512) (512, 512)\n",
      "transformer.layers[1].norm1.weight (512,) (512,)\n",
      "transformer.layers[1].norm1.bias (512,) (512,)\n",
      "transformer.layers[1].norm2.weight (512,) (512,)\n",
      "transformer.layers[1].norm2.bias (512,) (512,)\n",
      "transformer.layers[2].linear1.weight (2048, 512) (2048, 512)\n",
      "transformer.layers[2].linear2.weight (512, 2048) (512, 2048)\n",
      "transformer.layers[2].self_attn.in_proj.weight (1536, 512) (1536, 512)\n",
      "transformer.layers[2].self_attn.out_proj.weight (512, 512) (512, 512)\n",
      "transformer.layers[2].norm1.weight (512,) (512,)\n",
      "transformer.layers[2].norm1.bias (512,) (512,)\n",
      "transformer.layers[2].norm2.weight (512,) (512,)\n",
      "transformer.layers[2].norm2.bias (512,) (512,)\n",
      "transformer.layers[3].linear1.weight (2048, 512) (2048, 512)\n",
      "transformer.layers[3].linear2.weight (512, 2048) (512, 2048)\n",
      "transformer.layers[3].self_attn.in_proj.weight (1536, 512) (1536, 512)\n",
      "transformer.layers[3].self_attn.out_proj.weight (512, 512) (512, 512)\n",
      "transformer.layers[3].norm1.weight (512,) (512,)\n",
      "transformer.layers[3].norm1.bias (512,) (512,)\n",
      "transformer.layers[3].norm2.weight (512,) (512,)\n",
      "transformer.layers[3].norm2.bias (512,) (512,)\n",
      "transformer.layers[4].linear1.weight (2048, 512) (2048, 512)\n",
      "transformer.layers[4].linear2.weight (512, 2048) (512, 2048)\n",
      "transformer.layers[4].self_attn.in_proj.weight (1536, 512) (1536, 512)\n",
      "transformer.layers[4].self_attn.out_proj.weight (512, 512) (512, 512)\n",
      "transformer.layers[4].norm1.weight (512,) (512,)\n",
      "transformer.layers[4].norm1.bias (512,) (512,)\n",
      "transformer.layers[4].norm2.weight (512,) (512,)\n",
      "transformer.layers[4].norm2.bias (512,) (512,)\n",
      "transformer.layers[5].linear1.weight (2048, 512) (2048, 512)\n",
      "transformer.layers[5].linear2.weight (512, 2048) (512, 2048)\n",
      "transformer.layers[5].self_attn.in_proj.weight (1536, 512) (1536, 512)\n",
      "transformer.layers[5].self_attn.out_proj.weight (512, 512) (512, 512)\n",
      "transformer.layers[5].norm1.weight (512,) (512,)\n",
      "transformer.layers[5].norm1.bias (512,) (512,)\n",
      "transformer.layers[5].norm2.weight (512,) (512,)\n",
      "transformer.layers[5].norm2.bias (512,) (512,)\n",
      "transformer.layers[6].linear1.weight (2048, 512) (2048, 512)\n",
      "transformer.layers[6].linear2.weight (512, 2048) (512, 2048)\n",
      "transformer.layers[6].self_attn.in_proj.weight (1536, 512) (1536, 512)\n",
      "transformer.layers[6].self_attn.out_proj.weight (512, 512) (512, 512)\n",
      "transformer.layers[6].norm1.weight (512,) (512,)\n",
      "transformer.layers[6].norm1.bias (512,) (512,)\n",
      "transformer.layers[6].norm2.weight (512,) (512,)\n",
      "transformer.layers[6].norm2.bias (512,) (512,)\n",
      "transformer.layers[7].linear1.weight (2048, 512) (2048, 512)\n",
      "transformer.layers[7].linear2.weight (512, 2048) (512, 2048)\n",
      "transformer.layers[7].self_attn.in_proj.weight (1536, 512) (1536, 512)\n",
      "transformer.layers[7].self_attn.out_proj.weight (512, 512) (512, 512)\n",
      "transformer.layers[7].norm1.weight (512,) (512,)\n",
      "transformer.layers[7].norm1.bias (512,) (512,)\n",
      "transformer.layers[7].norm2.weight (512,) (512,)\n",
      "transformer.layers[7].norm2.bias (512,) (512,)\n",
      "(101, 512)\n",
      "Ours: Traced<ShapedArray(float32[10])>with<BatchTrace> with\n",
      "  val = Array([[ 0.4548429 , -0.40857568,  1.28716   ,  0.92222464, -1.7021128 ,\n",
      "        -2.3131115 , -0.44596407,  0.70908743,  0.11893629,  0.8162175 ]],      dtype=float32)\n",
      "  batch_dim = 0\n",
      "(101, 512)\n",
      "(1536, 512)\n",
      "Ours: Traced<ShapedArray(float32[10])>with<BatchTrace> with\n",
      "  val = Array([[ 0.45894626, -0.41413367,  1.2840986 ,  0.9236786 , -1.6979691 ,\n",
      "        -2.3188083 , -0.43853807,  0.7112526 ,  0.11691556,  0.820251  ]],      dtype=float32)\n",
      "  batch_dim = 0\n",
      "Ours: Traced<ShapedArray(float32[10])>with<BatchTrace> with\n",
      "  val = Array([[ 0.45947325, -0.4150821 ,  1.28072   ,  0.9161922 , -1.697064  ,\n",
      "        -2.3193038 , -0.438788  ,  0.7090815 ,  0.11934245,  0.82415944]],      dtype=float32)\n",
      "  batch_dim = 0\n",
      "Ours: Traced<ShapedArray(float32[10])>with<BatchTrace> with\n",
      "  val = Array([[ 0.4596978 , -0.4194129 ,  1.280018  ,  0.91411126, -1.7058324 ,\n",
      "        -2.3161838 , -0.43445662,  0.7137222 ,  0.11496932,  0.8208151 ]],      dtype=float32)\n",
      "  batch_dim = 0\n",
      "Ours: Traced<ShapedArray(float32[10])>with<BatchTrace> with\n",
      "  val = Array([[ 0.45202252, -0.41908243,  1.2795521 ,  0.91781485, -1.7122236 ,\n",
      "        -2.3189137 , -0.4298614 ,  0.7112791 ,  0.10821734,  0.81271404]],      dtype=float32)\n",
      "  batch_dim = 0\n",
      "Ours: Traced<ShapedArray(float32[10])>with<BatchTrace> with\n",
      "  val = Array([[ 0.45377573, -0.41940224,  1.2804825 ,  0.9254968 , -1.7146976 ,\n",
      "        -2.3202615 , -0.42500824,  0.70537734,  0.11407146,  0.81059533]],      dtype=float32)\n",
      "  batch_dim = 0\n",
      "Ours: Traced<ShapedArray(float32[10])>with<BatchTrace> with\n",
      "  val = Array([[ 0.44881728, -0.41633815,  1.2827806 ,  0.92384684, -1.7108546 ,\n",
      "        -2.3217514 , -0.42522547,  0.70402837,  0.11077451,  0.8083143 ]],      dtype=float32)\n",
      "  batch_dim = 0\n",
      "Ours: Traced<ShapedArray(float32[10])>with<BatchTrace> with\n",
      "  val = Array([[ 0.4547935 , -0.41385597,  1.2851268 ,  0.9218753 , -1.7110991 ,\n",
      "        -2.326159  , -0.4265656 ,  0.70622456,  0.11232214,  0.81172466]],      dtype=float32)\n",
      "  batch_dim = 0\n",
      "Theirs tensor([[ 0.4548, -0.4086,  1.2872,  ..., -1.8804, -0.7828,  0.4512],\n",
      "        [ 0.0718,  0.0130, -0.8903,  ...,  0.9041, -3.2616,  0.3515],\n",
      "        [-0.4082, -0.6130, -1.4529,  ...,  0.4205,  0.6253,  0.1094],\n",
      "        ...,\n",
      "        [ 0.3770,  0.2050,  0.5943,  ..., -1.0828, -0.1976, -0.0856],\n",
      "        [ 0.2255,  0.8868, -0.8096,  ...,  0.6884, -0.6223,  0.7129],\n",
      "        [ 0.8894,  0.3866, -1.1461,  ..., -0.0915,  0.5530,  0.4443]])\n",
      "Their shape of query: torch.Size([1, 101, 512])\n",
      "Theirs tensor([[ 0.4589, -0.4141,  1.2841,  ..., -1.8780, -0.7771,  0.4488],\n",
      "        [ 0.0698,  0.0110, -0.8881,  ...,  0.9027, -3.2561,  0.3512],\n",
      "        [-0.4045, -0.6161, -1.4541,  ...,  0.4218,  0.6357,  0.1092],\n",
      "        ...,\n",
      "        [ 0.3799,  0.2038,  0.5965,  ..., -1.0842, -0.1965, -0.0853],\n",
      "        [ 0.2246,  0.8829, -0.8124,  ...,  0.6890, -0.6213,  0.7093],\n",
      "        [ 0.8909,  0.3887, -1.1476,  ..., -0.0933,  0.5548,  0.4435]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Their shape of query: torch.Size([1, 101, 512])\n",
      "Theirs tensor([[ 0.4595, -0.4151,  1.2807,  ..., -1.8788, -0.7743,  0.4449],\n",
      "        [ 0.0711,  0.0114, -0.8920,  ...,  0.9045, -3.2543,  0.3545],\n",
      "        [-0.4057, -0.6173, -1.4568,  ...,  0.4226,  0.6310,  0.1086],\n",
      "        ...,\n",
      "        [ 0.3832,  0.2080,  0.5982,  ..., -1.0878, -0.1957, -0.0847],\n",
      "        [ 0.2263,  0.8844, -0.8128,  ...,  0.6906, -0.6186,  0.7103],\n",
      "        [ 0.8931,  0.3890, -1.1468,  ..., -0.0990,  0.5571,  0.4453]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Their shape of query: torch.Size([1, 101, 512])\n",
      "Theirs tensor([[ 4.5970e-01, -4.1941e-01,  1.2800e+00,  ..., -1.8751e+00,\n",
      "         -7.7046e-01,  4.4888e-01],\n",
      "        [ 7.0660e-02,  2.3609e-03, -8.9579e-01,  ...,  9.0770e-01,\n",
      "         -3.2544e+00,  3.5682e-01],\n",
      "        [-4.0550e-01, -6.1847e-01, -1.4562e+00,  ...,  4.2305e-01,\n",
      "          6.2855e-01,  1.1064e-01],\n",
      "        ...,\n",
      "        [ 3.8118e-01,  2.0662e-01,  5.9797e-01,  ..., -1.0844e+00,\n",
      "         -1.9574e-01, -8.2861e-02],\n",
      "        [ 2.2464e-01,  8.8144e-01, -8.1120e-01,  ...,  6.9394e-01,\n",
      "         -6.1633e-01,  7.0844e-01],\n",
      "        [ 8.9120e-01,  3.8922e-01, -1.1460e+00,  ..., -9.9006e-02,\n",
      "          5.5580e-01,  4.4925e-01]], grad_fn=<SliceBackward0>)\n",
      "Their shape of query: torch.Size([1, 101, 512])\n",
      "Theirs tensor([[ 4.5202e-01, -4.1908e-01,  1.2796e+00,  ..., -1.8733e+00,\n",
      "         -7.6962e-01,  4.4636e-01],\n",
      "        [ 6.3202e-02,  1.7770e-03, -8.9546e-01,  ...,  9.0810e-01,\n",
      "         -3.2530e+00,  3.5632e-01],\n",
      "        [-4.0725e-01, -6.2008e-01, -1.4548e+00,  ...,  4.2462e-01,\n",
      "          6.3011e-01,  1.1277e-01],\n",
      "        ...,\n",
      "        [ 3.7941e-01,  2.0346e-01,  5.9878e-01,  ..., -1.0768e+00,\n",
      "         -1.9656e-01, -8.4922e-02],\n",
      "        [ 2.2504e-01,  8.8080e-01, -8.1359e-01,  ...,  7.0077e-01,\n",
      "         -6.1534e-01,  7.0951e-01],\n",
      "        [ 8.8957e-01,  3.8875e-01, -1.1470e+00,  ..., -1.0074e-01,\n",
      "          5.5485e-01,  4.4916e-01]], grad_fn=<SliceBackward0>)\n",
      "Their shape of query: torch.Size([1, 101, 512])\n",
      "Theirs tensor([[ 4.5378e-01, -4.1940e-01,  1.2805e+00,  ..., -1.8780e+00,\n",
      "         -7.6511e-01,  4.4609e-01],\n",
      "        [ 6.5189e-02,  1.6655e-03, -8.9099e-01,  ...,  9.0683e-01,\n",
      "         -3.2520e+00,  3.5628e-01],\n",
      "        [-4.0900e-01, -6.1799e-01, -1.4578e+00,  ...,  4.2067e-01,\n",
      "          6.3420e-01,  1.1559e-01],\n",
      "        ...,\n",
      "        [ 3.8034e-01,  2.0500e-01,  5.9393e-01,  ..., -1.0783e+00,\n",
      "         -1.9379e-01, -8.4889e-02],\n",
      "        [ 2.2810e-01,  8.7827e-01, -8.1411e-01,  ...,  6.9915e-01,\n",
      "         -6.1345e-01,  7.0899e-01],\n",
      "        [ 8.9256e-01,  3.8823e-01, -1.1483e+00,  ..., -9.8638e-02,\n",
      "          5.5587e-01,  4.5014e-01]], grad_fn=<SliceBackward0>)\n",
      "Their shape of query: torch.Size([1, 101, 512])\n",
      "Theirs tensor([[ 0.4488, -0.4163,  1.2828,  ..., -1.8768, -0.7673,  0.4482],\n",
      "        [ 0.0651,  0.0081, -0.8901,  ...,  0.9023, -3.2535,  0.3521],\n",
      "        [-0.4082, -0.6180, -1.4572,  ...,  0.4226,  0.6315,  0.1155],\n",
      "        ...,\n",
      "        [ 0.3829,  0.2033,  0.5951,  ..., -1.0805, -0.1924, -0.0822],\n",
      "        [ 0.2268,  0.8778, -0.8164,  ...,  0.6970, -0.6139,  0.7119],\n",
      "        [ 0.8922,  0.3870, -1.1491,  ..., -0.1009,  0.5568,  0.4517]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Their shape of query: torch.Size([1, 101, 512])\n",
      "Theirs tensor([[ 0.4548, -0.4139,  1.2851,  ..., -1.8735, -0.7592,  0.4520],\n",
      "        [ 0.0667,  0.0103, -0.8898,  ...,  0.9016, -3.2510,  0.3536],\n",
      "        [-0.4077, -0.6128, -1.4514,  ...,  0.4220,  0.6374,  0.1160],\n",
      "        ...,\n",
      "        [ 0.3831,  0.2053,  0.5949,  ..., -1.0835, -0.1893, -0.0840],\n",
      "        [ 0.2266,  0.8798, -0.8176,  ...,  0.6985, -0.6126,  0.7097],\n",
      "        [ 0.8926,  0.3907, -1.1466,  ..., -0.1009,  0.5586,  0.4573]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Their shape of query: torch.Size([1, 101, 512])\n",
      "[ 0.45145637  0.07542637 -0.41175947  0.6920394   0.51967114 -2.1138089\n",
      "  1.6881373   0.366993    0.24717776  0.902422  ]\n",
      "tensor([ 0.4515,  0.0628, -0.4137,  0.7001,  0.5193, -2.1200,  1.6769,  0.3845,\n",
      "         0.2246,  0.8916], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.tree_util as jtu\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def copy_weights(path, x):\n",
    "    path = jtu.keystr(path)[1:]\n",
    "    # if \"[0].weight\"\n",
    "    \n",
    "    if \"layers[\" in path:\n",
    "        new_path = path.replace(\"[\", \".\").replace(\"]\", \"\")\n",
    "\n",
    "        if \".weight\" in path and \"weights\" not in path:\n",
    "            if \"in_proj\" in path:\n",
    "                new_path = new_path.replace(\"in_proj.weight\", \"in_proj_weight\")\n",
    "            print(f\"{path} {x.shape} {their_params[new_path].shape}\")\n",
    "            return their_params[new_path]\n",
    "        elif \"bias\" in path:\n",
    "            print(f\"{path} {x.shape} {their_params[new_path].shape}\")\n",
    "            return their_params[new_path]\n",
    "    return x \n",
    "\n",
    "    \n",
    "jax_transformer = jtu.tree_map_with_path(copy_weights, jax_transformer)\n",
    "\n",
    "import torch\n",
    "our_x = jax.random.normal(jax.random.key(1), shape=(1, 512, 101))\n",
    "their_x= torch.Tensor(numpy.array(our_x))\n",
    "\n",
    "our_res = jax.vmap(jax_transformer)(our_x)\n",
    "their_res = transformer(their_x)\n",
    "\n",
    "print(our_res[0][0,0,:10])\n",
    "print(their_res[0][0,0,:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 256, 16)\n",
      "first_layer.conv.conv.weight (256, 512, 7) (256, 512, 7)\n",
      "first_layer.conv.conv.bias\n",
      "blocks[0][0][0].blocks[0].conv.conv.weight (64, 128, 3) (64, 128, 3)\n",
      "blocks[0][0][0].blocks[0].conv.conv.bias\n",
      "blocks[0][0][0].blocks[1].conv.conv.weight (128, 64, 1) (128, 64, 1)\n",
      "blocks[0][0][0].blocks[1].conv.conv.bias\n",
      "blocks[0][1].convtr.convtr.weight (128, 256, 16) (256, 128, 16)\n",
      "blocks[0][1].convtr.convtr.bias\n",
      "blocks[1][0][0].blocks[0].conv.conv.weight (32, 64, 3) (32, 64, 3)\n",
      "blocks[1][0][0].blocks[0].conv.conv.bias\n",
      "blocks[1][0][0].blocks[1].conv.conv.weight (64, 32, 1) (64, 32, 1)\n",
      "blocks[1][0][0].blocks[1].conv.conv.bias\n",
      "blocks[1][1].convtr.convtr.weight (64, 128, 12) (128, 64, 12)\n",
      "blocks[1][1].convtr.convtr.bias\n",
      "last_layer.conv.conv.weight (1, 64, 3) (1, 64, 3)\n",
      "last_layer.conv.conv.bias\n",
      "(128, 256, 16)\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.tree_util as jtu\n",
    "import jax.numpy as jnp\n",
    "\n",
    "dec_matches = [\n",
    "    (\"first_layer.conv.conv.weight\", \"model.0.conv.conv.weight\"),\n",
    "    (\"first_layer.conv.conv.bias\", \"model.0.conv.conv.bias\"),\n",
    "    (\"blocks[0][1].convtr.convtr.weight\", \"model.2.convtr.convtr.weight\"),\n",
    "    (\"blocks[0][1].convtr.convtr.bias\", \"model.2.convtr.convtr.bias\"),\n",
    "    (\"blocks[0][0][0].blocks[1].conv.conv.weight\",\"model.3.block.3.conv.conv.weight\"),\n",
    "    (\"blocks[0][0][0].blocks[1].conv.conv.bias\",\"model.3.block.3.conv.conv.bias\"),    (\"blocks[0][0][0].blocks[0].conv.conv.weight\",\"model.3.block.1.conv.conv.weight\"),\n",
    "    (\"blocks[0][0][0].blocks[0].conv.conv.bias\",\"model.3.block.1.conv.conv.bias\"),\n",
    "    (\"blocks[1][1].convtr.convtr.weight\", \"model.5.convtr.convtr.weight\"),\n",
    "    (\"blocks[1][1].convtr.convtr.bias\", \"model.5.convtr.convtr.bias\"),\n",
    "    (\"blocks[1][0][0].blocks[1].conv.conv.weight\",\"model.6.block.3.conv.conv.weight\"),\n",
    "    (\"blocks[1][0][0].blocks[1].conv.conv.bias\",\"model.6.block.3.conv.conv.bias\"),    (\"blocks[1][0][0].blocks[0].conv.conv.weight\",\"model.6.block.1.conv.conv.weight\"),\n",
    "    (\"blocks[1][0][0].blocks[0].conv.conv.bias\",\"model.6.block.1.conv.conv.bias\"),\n",
    "    (\"last_layer.conv.conv.weight\", \"model.8.conv.conv.weight\"),\n",
    "    (\"last_layer.conv.conv.bias\", \"model.8.conv.conv.bias\"),\n",
    "]\n",
    "\n",
    "enc_matches = [\n",
    "    (\"first_layer.conv.conv.weight\", \"model.0.conv.conv.weight\"),\n",
    "    (\"first_layer.conv.conv.bias\", \"model.0.conv.conv.bias\"),\n",
    "    (\"blocks[0][0][0].blocks[1].conv.conv.weight\",\"model.1.block.3.conv.conv.weight\"),\n",
    "    (\"blocks[0][0][0].blocks[1].conv.conv.bias\",\"model.1.block.3.conv.conv.bias\"),    (\"blocks[0][0][0].blocks[0].conv.conv.weight\",\"model.1.block.1.conv.conv.weight\"),\n",
    "    (\"blocks[0][0][0].blocks[0].conv.conv.bias\",\"model.1.block.1.conv.conv.bias\"),\n",
    "    (\"blocks[0][1].conv.conv.weight\", \"model.3.conv.conv.weight\"),\n",
    "    (\"blocks[0][1].conv.conv.bias\", \"model.3.conv.conv.bias\"),\n",
    "    (\"blocks[1][0][0].blocks[1].conv.conv.weight\",\"model.4.block.3.conv.conv.weight\"),\n",
    "    (\"blocks[1][0][0].blocks[1].conv.conv.bias\",\"model.4.block.3.conv.conv.bias\"),    (\"blocks[1][0][0].blocks[0].conv.conv.weight\",\"model.4.block.1.conv.conv.weight\"),\n",
    "    (\"blocks[1][0][0].blocks[0].conv.conv.bias\",\"model.4.block.1.conv.conv.bias\"),\n",
    "    (\"blocks[1][1].conv.conv.weight\", \"model.6.conv.conv.weight\"),\n",
    "    (\"blocks[1][1].conv.conv.bias\", \"model.6.conv.conv.bias\"),    \n",
    "    (\"last_layer.conv.conv.weight\", \"model.8.conv.conv.weight\"),\n",
    "    (\"last_layer.conv.conv.bias\", \"model.8.conv.conv.bias\"),\n",
    "]\n",
    "print(jax_decoder.blocks[0][1].convtr.convtr.weight.shape)\n",
    "\n",
    "dec_matches = {k: v for k, v in dec_matches}\n",
    "enc_matches = {k: v for k, v in enc_matches}\n",
    "\n",
    "def copy_weights(path, x):\n",
    "    path = jtu.keystr(path)[1:]\n",
    "    if path in enc_matches.keys() and \"weight\" in path:\n",
    "        return their_enc_params[enc_matches[path]]\n",
    "    elif path in enc_matches.keys() and \"bias\" in path:\n",
    "        return jnp.expand_dims(their_enc_params[enc_matches[path]], -1)\n",
    "    return x \n",
    "\n",
    "def dec_copy_weights(path, x):\n",
    "    # print(path)\n",
    "\n",
    "    path = jtu.keystr(path)[1:]\n",
    "    if path in dec_matches.keys() and \"weight\" in path:\n",
    "        print(f\"{path} {x.shape} {their_dec_params[dec_matches[path]].shape}\")\n",
    "        if \"convtr\" in path:\n",
    "            return jnp.permute_dims(their_dec_params[dec_matches[path]], (1, 0, 2))\n",
    "        return their_dec_params[dec_matches[path]]\n",
    "    elif path in dec_matches.keys() and \"bias\" in path:\n",
    "        print(path)\n",
    "        return jnp.expand_dims(their_dec_params[dec_matches[path]], -1)\n",
    "    # print(path)\n",
    "    return x \n",
    "\n",
    "    \n",
    "jax_encoder = jtu.tree_map_with_path(copy_weights, jax_encoder)\n",
    "jax_decoder = jtu.tree_map_with_path(dec_copy_weights, jax_decoder)\n",
    "print(jax_decoder.blocks[0][1].convtr.convtr.weight.shape)\n",
    "\n",
    "import torch\n",
    "our_x = jax.random.normal(jax.random.key(1), shape=(1, 1, 960))\n",
    "their_x= torch.Tensor(numpy.array(our_x))\n",
    "\n",
    "our_res = jax.vmap(jax_encoder)(our_x)\n",
    "their_res = encoder(their_x)\n",
    "\n",
    "print(our_res.shape)\n",
    "print(their_res.shape)\n",
    "\n",
    "their_res = decoder(their_res)\n",
    "our_res = jax.vmap(jax_decoder)(our_res)\n",
    "\n",
    "print(our_res.shape)\n",
    "print(their_res.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Their mean size: torch.Size([3, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "Their sample size: torch.Size([60, 2])\n",
      "Their bucket size: torch.Size([60])\n",
      "newmeans size : torch.Size([3, 2])\n",
      "Their bins size: torch.Size([3])\n",
      "Poute torch.Size([60, 2])\n",
      "(tensor([[0., 0.],\n",
      "        [1., 1.],\n",
      "        [2., 2.]]), tensor([20, 20, 20]))\n",
      "Our mean size: (3, 2)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "Our sample size: (60, 2)\n",
      "Our bucket size: (60,)\n",
      "Our bins size: (3,)\n",
      "(Array([[2., 2.],\n",
      "       [1., 1.],\n",
      "       [0., 0.]], dtype=float32), Array([20, 20, 20], dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import numpy\n",
    "import torch\n",
    "import jax.numpy as jnp\n",
    "from moshi.moshi.quantization.core_vq import _run_kmeans\n",
    "from moshi_jax.moshi_jax.quantization.core_vq import _run_kmeans as jax_run_kmeans\n",
    "\n",
    "our_x = jnp.concat([jnp.ones((20, 2)) * 2, jnp.ones((20, 2)), jnp.zeros((20, 2))])\n",
    "x = torch.from_numpy(numpy.array(our_x))\n",
    "\n",
    "print(_run_kmeans(x, 3))\n",
    "print(jax_run_kmeans(our_x, 3, key=jax.random.key(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speechtok",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
