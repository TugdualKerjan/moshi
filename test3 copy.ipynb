{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tugdual/miniconda3/envs/xtts/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/tugdual/moshi/moshi_jax/moshi_jax/quantization/core_vq.py:283: UserWarning: A JAX array is being set as static! This can result in unexpected behavior and is usually a mistake to do.\n",
      "  self._codebook = EuclideanCodebook(\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import torch\n",
    "import numpy\n",
    "from huggingface_hub import hf_hub_download\n",
    "import jax\n",
    "\n",
    "from moshi.moshi.models.loaders import get_mimi\n",
    "from moshi_jax.moshi_jax.models.loaders import (\n",
    "    get_mimi as get_jax_mimi,\n",
    ")\n",
    "\n",
    "device = torch.get_default_device()\n",
    "mimi_weight = hf_hub_download(\n",
    "    \"kyutai/moshiko-pytorch-bf16\", \"tokenizer-e351c8d8-checkpoint125.safetensors\"\n",
    ")\n",
    "model = get_mimi(mimi_weight, device='cpu')\n",
    "model.set_num_codebooks(8)\n",
    "jax_model = get_jax_mimi(\"pouet\", jax.random.key(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.channels\n",
      "encoder.dimension\n",
      "encoder.n_filters\n",
      "encoder.ratios[0]\n",
      "encoder.ratios[1]\n",
      "encoder.ratios[2]\n",
      "encoder.ratios[3]\n",
      "encoder.n_residual_layers\n",
      "encoder.hop_length\n",
      "encoder.n_blocks\n",
      "encoder.disable_norm_outer_blocks\n",
      "encoder.act\n",
      "encoder.first_layer.conv.conv.weight\n",
      "encoder.first_layer.conv.conv.bias\n",
      "encoder.first_layer.conv.norm_type\n",
      "encoder.first_layer.causal\n",
      "encoder.first_layer.pad_mode\n",
      "encoder.blocks[0][0][0].act\n",
      "encoder.blocks[0][0][0].blocks[0].conv.conv.weight\n",
      "encoder.blocks[0][0][0].blocks[0].conv.conv.bias\n",
      "encoder.blocks[0][0][0].blocks[0].conv.norm_type\n",
      "encoder.blocks[0][0][0].blocks[0].causal\n",
      "encoder.blocks[0][0][0].blocks[0].pad_mode\n",
      "encoder.blocks[0][0][0].blocks[1].conv.conv.weight\n",
      "encoder.blocks[0][0][0].blocks[1].conv.conv.bias\n",
      "encoder.blocks[0][0][0].blocks[1].conv.norm_type\n",
      "encoder.blocks[0][0][0].blocks[1].causal\n",
      "encoder.blocks[0][0][0].blocks[1].pad_mode\n",
      "encoder.blocks[0][0][0].true_skip\n",
      "encoder.blocks[0][1].conv.conv.weight\n",
      "encoder.blocks[0][1].conv.conv.bias\n",
      "encoder.blocks[0][1].conv.norm_type\n",
      "encoder.blocks[0][1].causal\n",
      "encoder.blocks[0][1].pad_mode\n",
      "encoder.blocks[1][0][0].act\n",
      "encoder.blocks[1][0][0].blocks[0].conv.conv.weight\n",
      "encoder.blocks[1][0][0].blocks[0].conv.conv.bias\n",
      "encoder.blocks[1][0][0].blocks[0].conv.norm_type\n",
      "encoder.blocks[1][0][0].blocks[0].causal\n",
      "encoder.blocks[1][0][0].blocks[0].pad_mode\n",
      "encoder.blocks[1][0][0].blocks[1].conv.conv.weight\n",
      "encoder.blocks[1][0][0].blocks[1].conv.conv.bias\n",
      "encoder.blocks[1][0][0].blocks[1].conv.norm_type\n",
      "encoder.blocks[1][0][0].blocks[1].causal\n",
      "encoder.blocks[1][0][0].blocks[1].pad_mode\n",
      "encoder.blocks[1][0][0].true_skip\n",
      "encoder.blocks[1][1].conv.conv.weight\n",
      "encoder.blocks[1][1].conv.conv.bias\n",
      "encoder.blocks[1][1].conv.norm_type\n",
      "encoder.blocks[1][1].causal\n",
      "encoder.blocks[1][1].pad_mode\n",
      "encoder.blocks[2][0][0].act\n",
      "encoder.blocks[2][0][0].blocks[0].conv.conv.weight\n",
      "encoder.blocks[2][0][0].blocks[0].conv.conv.bias\n",
      "encoder.blocks[2][0][0].blocks[0].conv.norm_type\n",
      "encoder.blocks[2][0][0].blocks[0].causal\n",
      "encoder.blocks[2][0][0].blocks[0].pad_mode\n",
      "encoder.blocks[2][0][0].blocks[1].conv.conv.weight\n",
      "encoder.blocks[2][0][0].blocks[1].conv.conv.bias\n",
      "encoder.blocks[2][0][0].blocks[1].conv.norm_type\n",
      "encoder.blocks[2][0][0].blocks[1].causal\n",
      "encoder.blocks[2][0][0].blocks[1].pad_mode\n",
      "encoder.blocks[2][0][0].true_skip\n",
      "encoder.blocks[2][1].conv.conv.weight\n",
      "encoder.blocks[2][1].conv.conv.bias\n",
      "encoder.blocks[2][1].conv.norm_type\n",
      "encoder.blocks[2][1].causal\n",
      "encoder.blocks[2][1].pad_mode\n",
      "encoder.blocks[3][0][0].act\n",
      "encoder.blocks[3][0][0].blocks[0].conv.conv.weight\n",
      "encoder.blocks[3][0][0].blocks[0].conv.conv.bias\n",
      "encoder.blocks[3][0][0].blocks[0].conv.norm_type\n",
      "encoder.blocks[3][0][0].blocks[0].causal\n",
      "encoder.blocks[3][0][0].blocks[0].pad_mode\n",
      "encoder.blocks[3][0][0].blocks[1].conv.conv.weight\n",
      "encoder.blocks[3][0][0].blocks[1].conv.conv.bias\n",
      "encoder.blocks[3][0][0].blocks[1].conv.norm_type\n",
      "encoder.blocks[3][0][0].blocks[1].causal\n",
      "encoder.blocks[3][0][0].blocks[1].pad_mode\n",
      "encoder.blocks[3][0][0].true_skip\n",
      "encoder.blocks[3][1].conv.conv.weight\n",
      "encoder.blocks[3][1].conv.conv.bias\n",
      "encoder.blocks[3][1].conv.norm_type\n",
      "encoder.blocks[3][1].causal\n",
      "encoder.blocks[3][1].pad_mode\n",
      "encoder.last_layer.conv.conv.weight\n",
      "encoder.last_layer.conv.conv.bias\n",
      "encoder.last_layer.conv.norm_type\n",
      "encoder.last_layer.causal\n",
      "encoder.last_layer.pad_mode\n",
      "decoder.channels\n",
      "decoder.dimension\n",
      "decoder.n_filters\n",
      "decoder.ratios[0]\n",
      "decoder.ratios[1]\n",
      "decoder.ratios[2]\n",
      "decoder.ratios[3]\n",
      "decoder.n_residual_layers\n",
      "decoder.hop_length\n",
      "decoder.n_blocks\n",
      "decoder.disable_norm_outer_blocks\n",
      "decoder.act\n",
      "decoder.first_layer.conv.conv.weight\n",
      "decoder.first_layer.conv.conv.bias\n",
      "decoder.first_layer.conv.norm_type\n",
      "decoder.first_layer.causal\n",
      "decoder.first_layer.pad_mode\n",
      "decoder.blocks[0][0].convtr.convtr.weight\n",
      "decoder.blocks[0][0].convtr.convtr.bias\n",
      "decoder.blocks[0][0].convtr.norm_type\n",
      "decoder.blocks[0][0].causal\n",
      "decoder.blocks[0][0].trim_right_ratio\n",
      "decoder.blocks[0][1][0].act\n",
      "decoder.blocks[0][1][0].blocks[0].conv.conv.weight\n",
      "decoder.blocks[0][1][0].blocks[0].conv.conv.bias\n",
      "decoder.blocks[0][1][0].blocks[0].conv.norm_type\n",
      "decoder.blocks[0][1][0].blocks[0].causal\n",
      "decoder.blocks[0][1][0].blocks[0].pad_mode\n",
      "decoder.blocks[0][1][0].blocks[1].conv.conv.weight\n",
      "decoder.blocks[0][1][0].blocks[1].conv.conv.bias\n",
      "decoder.blocks[0][1][0].blocks[1].conv.norm_type\n",
      "decoder.blocks[0][1][0].blocks[1].causal\n",
      "decoder.blocks[0][1][0].blocks[1].pad_mode\n",
      "decoder.blocks[0][1][0].true_skip\n",
      "decoder.blocks[1][0].convtr.convtr.weight\n",
      "decoder.blocks[1][0].convtr.convtr.bias\n",
      "decoder.blocks[1][0].convtr.norm_type\n",
      "decoder.blocks[1][0].causal\n",
      "decoder.blocks[1][0].trim_right_ratio\n",
      "decoder.blocks[1][1][0].act\n",
      "decoder.blocks[1][1][0].blocks[0].conv.conv.weight\n",
      "decoder.blocks[1][1][0].blocks[0].conv.conv.bias\n",
      "decoder.blocks[1][1][0].blocks[0].conv.norm_type\n",
      "decoder.blocks[1][1][0].blocks[0].causal\n",
      "decoder.blocks[1][1][0].blocks[0].pad_mode\n",
      "decoder.blocks[1][1][0].blocks[1].conv.conv.weight\n",
      "decoder.blocks[1][1][0].blocks[1].conv.conv.bias\n",
      "decoder.blocks[1][1][0].blocks[1].conv.norm_type\n",
      "decoder.blocks[1][1][0].blocks[1].causal\n",
      "decoder.blocks[1][1][0].blocks[1].pad_mode\n",
      "decoder.blocks[1][1][0].true_skip\n",
      "decoder.blocks[2][0].convtr.convtr.weight\n",
      "decoder.blocks[2][0].convtr.convtr.bias\n",
      "decoder.blocks[2][0].convtr.norm_type\n",
      "decoder.blocks[2][0].causal\n",
      "decoder.blocks[2][0].trim_right_ratio\n",
      "decoder.blocks[2][1][0].act\n",
      "decoder.blocks[2][1][0].blocks[0].conv.conv.weight\n",
      "decoder.blocks[2][1][0].blocks[0].conv.conv.bias\n",
      "decoder.blocks[2][1][0].blocks[0].conv.norm_type\n",
      "decoder.blocks[2][1][0].blocks[0].causal\n",
      "decoder.blocks[2][1][0].blocks[0].pad_mode\n",
      "decoder.blocks[2][1][0].blocks[1].conv.conv.weight\n",
      "decoder.blocks[2][1][0].blocks[1].conv.conv.bias\n",
      "decoder.blocks[2][1][0].blocks[1].conv.norm_type\n",
      "decoder.blocks[2][1][0].blocks[1].causal\n",
      "decoder.blocks[2][1][0].blocks[1].pad_mode\n",
      "decoder.blocks[2][1][0].true_skip\n",
      "decoder.blocks[3][0].convtr.convtr.weight\n",
      "decoder.blocks[3][0].convtr.convtr.bias\n",
      "decoder.blocks[3][0].convtr.norm_type\n",
      "decoder.blocks[3][0].causal\n",
      "decoder.blocks[3][0].trim_right_ratio\n",
      "decoder.blocks[3][1][0].act\n",
      "decoder.blocks[3][1][0].blocks[0].conv.conv.weight\n",
      "decoder.blocks[3][1][0].blocks[0].conv.conv.bias\n",
      "decoder.blocks[3][1][0].blocks[0].conv.norm_type\n",
      "decoder.blocks[3][1][0].blocks[0].causal\n",
      "decoder.blocks[3][1][0].blocks[0].pad_mode\n",
      "decoder.blocks[3][1][0].blocks[1].conv.conv.weight\n",
      "decoder.blocks[3][1][0].blocks[1].conv.conv.bias\n",
      "decoder.blocks[3][1][0].blocks[1].conv.norm_type\n",
      "decoder.blocks[3][1][0].blocks[1].causal\n",
      "decoder.blocks[3][1][0].blocks[1].pad_mode\n",
      "decoder.blocks[3][1][0].true_skip\n",
      "decoder.last_layer.conv.conv.weight\n",
      "decoder.last_layer.conv.conv.bias\n",
      "decoder.last_layer.conv.norm_type\n",
      "decoder.last_layer.causal\n",
      "decoder.last_layer.pad_mode\n",
      "decoder_transformer.transformer.positional_embedding\n",
      "decoder_transformer.transformer.max_period\n",
      "decoder_transformer.transformer.positional_scale\n",
      "decoder_transformer.transformer.rope.max_period\n",
      "decoder_transformer.transformer.layers[0].self_attn.embed_dim\n",
      "decoder_transformer.transformer.layers[0].self_attn.causal\n",
      "decoder_transformer.transformer.layers[0].self_attn.context\n",
      "decoder_transformer.transformer.layers[0].self_attn.rope.max_period\n",
      "decoder_transformer.transformer.layers[0].self_attn.num_heads\n",
      "decoder_transformer.transformer.layers[0].skip_self_attn\n",
      "decoder_transformer.transformer.layers[0].activation\n",
      "decoder_transformer.transformer.layers[0].layer_scale_1.channel_last\n",
      "decoder_transformer.transformer.layers[0].layer_scale_2.channel_last\n",
      "decoder_transformer.transformer.layers[1].self_attn.embed_dim\n",
      "decoder_transformer.transformer.layers[1].self_attn.causal\n",
      "decoder_transformer.transformer.layers[1].self_attn.context\n",
      "decoder_transformer.transformer.layers[1].self_attn.rope.max_period\n",
      "decoder_transformer.transformer.layers[1].self_attn.num_heads\n",
      "decoder_transformer.transformer.layers[1].skip_self_attn\n",
      "decoder_transformer.transformer.layers[1].activation\n",
      "decoder_transformer.transformer.layers[1].layer_scale_1.channel_last\n",
      "decoder_transformer.transformer.layers[1].layer_scale_2.channel_last\n",
      "decoder_transformer.transformer.layers[2].self_attn.embed_dim\n",
      "decoder_transformer.transformer.layers[2].self_attn.causal\n",
      "decoder_transformer.transformer.layers[2].self_attn.context\n",
      "decoder_transformer.transformer.layers[2].self_attn.rope.max_period\n",
      "decoder_transformer.transformer.layers[2].self_attn.num_heads\n",
      "decoder_transformer.transformer.layers[2].skip_self_attn\n",
      "decoder_transformer.transformer.layers[2].activation\n",
      "decoder_transformer.transformer.layers[2].layer_scale_1.channel_last\n",
      "decoder_transformer.transformer.layers[2].layer_scale_2.channel_last\n",
      "decoder_transformer.transformer.layers[3].self_attn.embed_dim\n",
      "decoder_transformer.transformer.layers[3].self_attn.causal\n",
      "decoder_transformer.transformer.layers[3].self_attn.context\n",
      "decoder_transformer.transformer.layers[3].self_attn.rope.max_period\n",
      "decoder_transformer.transformer.layers[3].self_attn.num_heads\n",
      "decoder_transformer.transformer.layers[3].skip_self_attn\n",
      "decoder_transformer.transformer.layers[3].activation\n",
      "decoder_transformer.transformer.layers[3].layer_scale_1.channel_last\n",
      "decoder_transformer.transformer.layers[3].layer_scale_2.channel_last\n",
      "decoder_transformer.transformer.layers[4].self_attn.embed_dim\n",
      "decoder_transformer.transformer.layers[4].self_attn.causal\n",
      "decoder_transformer.transformer.layers[4].self_attn.context\n",
      "decoder_transformer.transformer.layers[4].self_attn.rope.max_period\n",
      "decoder_transformer.transformer.layers[4].self_attn.num_heads\n",
      "decoder_transformer.transformer.layers[4].skip_self_attn\n",
      "decoder_transformer.transformer.layers[4].activation\n",
      "decoder_transformer.transformer.layers[4].layer_scale_1.channel_last\n",
      "decoder_transformer.transformer.layers[4].layer_scale_2.channel_last\n",
      "decoder_transformer.transformer.layers[5].self_attn.embed_dim\n",
      "decoder_transformer.transformer.layers[5].self_attn.causal\n",
      "decoder_transformer.transformer.layers[5].self_attn.context\n",
      "decoder_transformer.transformer.layers[5].self_attn.rope.max_period\n",
      "decoder_transformer.transformer.layers[5].self_attn.num_heads\n",
      "decoder_transformer.transformer.layers[5].skip_self_attn\n",
      "decoder_transformer.transformer.layers[5].activation\n",
      "decoder_transformer.transformer.layers[5].layer_scale_1.channel_last\n",
      "decoder_transformer.transformer.layers[5].layer_scale_2.channel_last\n",
      "decoder_transformer.transformer.layers[6].self_attn.embed_dim\n",
      "decoder_transformer.transformer.layers[6].self_attn.causal\n",
      "decoder_transformer.transformer.layers[6].self_attn.context\n",
      "decoder_transformer.transformer.layers[6].self_attn.rope.max_period\n",
      "decoder_transformer.transformer.layers[6].self_attn.num_heads\n",
      "decoder_transformer.transformer.layers[6].skip_self_attn\n",
      "decoder_transformer.transformer.layers[6].activation\n",
      "decoder_transformer.transformer.layers[6].layer_scale_1.channel_last\n",
      "decoder_transformer.transformer.layers[6].layer_scale_2.channel_last\n",
      "decoder_transformer.transformer.layers[7].self_attn.embed_dim\n",
      "decoder_transformer.transformer.layers[7].self_attn.causal\n",
      "decoder_transformer.transformer.layers[7].self_attn.context\n",
      "decoder_transformer.transformer.layers[7].self_attn.rope.max_period\n",
      "decoder_transformer.transformer.layers[7].self_attn.num_heads\n",
      "decoder_transformer.transformer.layers[7].skip_self_attn\n",
      "decoder_transformer.transformer.layers[7].activation\n",
      "decoder_transformer.transformer.layers[7].layer_scale_1.channel_last\n",
      "decoder_transformer.transformer.layers[7].layer_scale_2.channel_last\n",
      "decoder_transformer.input_dimension\n",
      "decoder_transformer.output_dimensions[0]\n",
      "decoder_transformer.conv_layout\n",
      "encoder_transformer.transformer.positional_embedding\n",
      "encoder_transformer.transformer.max_period\n",
      "encoder_transformer.transformer.positional_scale\n",
      "encoder_transformer.transformer.rope.max_period\n",
      "encoder_transformer.transformer.layers[0].linear1.weight\n",
      "encoder_transformer.transformer.layers[0].linear2.weight\n",
      "encoder_transformer.transformer.layers[0].self_attn.embed_dim\n",
      "encoder_transformer.transformer.layers[0].self_attn.causal\n",
      "encoder_transformer.transformer.layers[0].self_attn.context\n",
      "encoder_transformer.transformer.layers[0].self_attn.rope.max_period\n",
      "encoder_transformer.transformer.layers[0].self_attn.num_heads\n",
      "encoder_transformer.transformer.layers[0].self_attn.weights_per_step\n",
      "encoder_transformer.transformer.layers[0].self_attn.in_proj.weight\n",
      "encoder_transformer.transformer.layers[0].self_attn.out_proj.weight\n",
      "encoder_transformer.transformer.layers[0].norm1.weight\n",
      "encoder_transformer.transformer.layers[0].norm1.bias\n",
      "encoder_transformer.transformer.layers[0].norm2.weight\n",
      "encoder_transformer.transformer.layers[0].norm2.bias\n",
      "encoder_transformer.transformer.layers[0].skip_self_attn\n",
      "encoder_transformer.transformer.layers[0].activation\n",
      "encoder_transformer.transformer.layers[0].layer_scale_1.scale\n",
      "encoder_transformer.transformer.layers[0].layer_scale_1.channel_last\n",
      "encoder_transformer.transformer.layers[0].layer_scale_2.scale\n",
      "encoder_transformer.transformer.layers[0].layer_scale_2.channel_last\n",
      "encoder_transformer.transformer.layers[0].weights_per_step\n",
      "encoder_transformer.transformer.layers[1].linear1.weight\n",
      "encoder_transformer.transformer.layers[1].linear2.weight\n",
      "encoder_transformer.transformer.layers[1].self_attn.embed_dim\n",
      "encoder_transformer.transformer.layers[1].self_attn.causal\n",
      "encoder_transformer.transformer.layers[1].self_attn.context\n",
      "encoder_transformer.transformer.layers[1].self_attn.rope.max_period\n",
      "encoder_transformer.transformer.layers[1].self_attn.num_heads\n",
      "encoder_transformer.transformer.layers[1].self_attn.weights_per_step\n",
      "encoder_transformer.transformer.layers[1].self_attn.in_proj.weight\n",
      "encoder_transformer.transformer.layers[1].self_attn.out_proj.weight\n",
      "encoder_transformer.transformer.layers[1].norm1.weight\n",
      "encoder_transformer.transformer.layers[1].norm1.bias\n",
      "encoder_transformer.transformer.layers[1].norm2.weight\n",
      "encoder_transformer.transformer.layers[1].norm2.bias\n",
      "encoder_transformer.transformer.layers[1].skip_self_attn\n",
      "encoder_transformer.transformer.layers[1].activation\n",
      "encoder_transformer.transformer.layers[1].layer_scale_1.scale\n",
      "encoder_transformer.transformer.layers[1].layer_scale_1.channel_last\n",
      "encoder_transformer.transformer.layers[1].layer_scale_2.scale\n",
      "encoder_transformer.transformer.layers[1].layer_scale_2.channel_last\n",
      "encoder_transformer.transformer.layers[1].weights_per_step\n",
      "encoder_transformer.transformer.layers[2].linear1.weight\n",
      "encoder_transformer.transformer.layers[2].linear2.weight\n",
      "encoder_transformer.transformer.layers[2].self_attn.embed_dim\n",
      "encoder_transformer.transformer.layers[2].self_attn.causal\n",
      "encoder_transformer.transformer.layers[2].self_attn.context\n",
      "encoder_transformer.transformer.layers[2].self_attn.rope.max_period\n",
      "encoder_transformer.transformer.layers[2].self_attn.num_heads\n",
      "encoder_transformer.transformer.layers[2].self_attn.weights_per_step\n",
      "encoder_transformer.transformer.layers[2].self_attn.in_proj.weight\n",
      "encoder_transformer.transformer.layers[2].self_attn.out_proj.weight\n",
      "encoder_transformer.transformer.layers[2].norm1.weight\n",
      "encoder_transformer.transformer.layers[2].norm1.bias\n",
      "encoder_transformer.transformer.layers[2].norm2.weight\n",
      "encoder_transformer.transformer.layers[2].norm2.bias\n",
      "encoder_transformer.transformer.layers[2].skip_self_attn\n",
      "encoder_transformer.transformer.layers[2].activation\n",
      "encoder_transformer.transformer.layers[2].layer_scale_1.scale\n",
      "encoder_transformer.transformer.layers[2].layer_scale_1.channel_last\n",
      "encoder_transformer.transformer.layers[2].layer_scale_2.scale\n",
      "encoder_transformer.transformer.layers[2].layer_scale_2.channel_last\n",
      "encoder_transformer.transformer.layers[2].weights_per_step\n",
      "encoder_transformer.transformer.layers[3].linear1.weight\n",
      "encoder_transformer.transformer.layers[3].linear2.weight\n",
      "encoder_transformer.transformer.layers[3].self_attn.embed_dim\n",
      "encoder_transformer.transformer.layers[3].self_attn.causal\n",
      "encoder_transformer.transformer.layers[3].self_attn.context\n",
      "encoder_transformer.transformer.layers[3].self_attn.rope.max_period\n",
      "encoder_transformer.transformer.layers[3].self_attn.num_heads\n",
      "encoder_transformer.transformer.layers[3].self_attn.weights_per_step\n",
      "encoder_transformer.transformer.layers[3].self_attn.in_proj.weight\n",
      "encoder_transformer.transformer.layers[3].self_attn.out_proj.weight\n",
      "encoder_transformer.transformer.layers[3].norm1.weight\n",
      "encoder_transformer.transformer.layers[3].norm1.bias\n",
      "encoder_transformer.transformer.layers[3].norm2.weight\n",
      "encoder_transformer.transformer.layers[3].norm2.bias\n",
      "encoder_transformer.transformer.layers[3].skip_self_attn\n",
      "encoder_transformer.transformer.layers[3].activation\n",
      "encoder_transformer.transformer.layers[3].layer_scale_1.scale\n",
      "encoder_transformer.transformer.layers[3].layer_scale_1.channel_last\n",
      "encoder_transformer.transformer.layers[3].layer_scale_2.scale\n",
      "encoder_transformer.transformer.layers[3].layer_scale_2.channel_last\n",
      "encoder_transformer.transformer.layers[3].weights_per_step\n",
      "encoder_transformer.transformer.layers[4].linear1.weight\n",
      "encoder_transformer.transformer.layers[4].linear2.weight\n",
      "encoder_transformer.transformer.layers[4].self_attn.embed_dim\n",
      "encoder_transformer.transformer.layers[4].self_attn.causal\n",
      "encoder_transformer.transformer.layers[4].self_attn.context\n",
      "encoder_transformer.transformer.layers[4].self_attn.rope.max_period\n",
      "encoder_transformer.transformer.layers[4].self_attn.num_heads\n",
      "encoder_transformer.transformer.layers[4].self_attn.weights_per_step\n",
      "encoder_transformer.transformer.layers[4].self_attn.in_proj.weight\n",
      "encoder_transformer.transformer.layers[4].self_attn.out_proj.weight\n",
      "encoder_transformer.transformer.layers[4].norm1.weight\n",
      "encoder_transformer.transformer.layers[4].norm1.bias\n",
      "encoder_transformer.transformer.layers[4].norm2.weight\n",
      "encoder_transformer.transformer.layers[4].norm2.bias\n",
      "encoder_transformer.transformer.layers[4].skip_self_attn\n",
      "encoder_transformer.transformer.layers[4].activation\n",
      "encoder_transformer.transformer.layers[4].layer_scale_1.scale\n",
      "encoder_transformer.transformer.layers[4].layer_scale_1.channel_last\n",
      "encoder_transformer.transformer.layers[4].layer_scale_2.scale\n",
      "encoder_transformer.transformer.layers[4].layer_scale_2.channel_last\n",
      "encoder_transformer.transformer.layers[4].weights_per_step\n",
      "encoder_transformer.transformer.layers[5].linear1.weight\n",
      "encoder_transformer.transformer.layers[5].linear2.weight\n",
      "encoder_transformer.transformer.layers[5].self_attn.embed_dim\n",
      "encoder_transformer.transformer.layers[5].self_attn.causal\n",
      "encoder_transformer.transformer.layers[5].self_attn.context\n",
      "encoder_transformer.transformer.layers[5].self_attn.rope.max_period\n",
      "encoder_transformer.transformer.layers[5].self_attn.num_heads\n",
      "encoder_transformer.transformer.layers[5].self_attn.weights_per_step\n",
      "encoder_transformer.transformer.layers[5].self_attn.in_proj.weight\n",
      "encoder_transformer.transformer.layers[5].self_attn.out_proj.weight\n",
      "encoder_transformer.transformer.layers[5].norm1.weight\n",
      "encoder_transformer.transformer.layers[5].norm1.bias\n",
      "encoder_transformer.transformer.layers[5].norm2.weight\n",
      "encoder_transformer.transformer.layers[5].norm2.bias\n",
      "encoder_transformer.transformer.layers[5].skip_self_attn\n",
      "encoder_transformer.transformer.layers[5].activation\n",
      "encoder_transformer.transformer.layers[5].layer_scale_1.scale\n",
      "encoder_transformer.transformer.layers[5].layer_scale_1.channel_last\n",
      "encoder_transformer.transformer.layers[5].layer_scale_2.scale\n",
      "encoder_transformer.transformer.layers[5].layer_scale_2.channel_last\n",
      "encoder_transformer.transformer.layers[5].weights_per_step\n",
      "encoder_transformer.transformer.layers[6].linear1.weight\n",
      "encoder_transformer.transformer.layers[6].linear2.weight\n",
      "encoder_transformer.transformer.layers[6].self_attn.embed_dim\n",
      "encoder_transformer.transformer.layers[6].self_attn.causal\n",
      "encoder_transformer.transformer.layers[6].self_attn.context\n",
      "encoder_transformer.transformer.layers[6].self_attn.rope.max_period\n",
      "encoder_transformer.transformer.layers[6].self_attn.num_heads\n",
      "encoder_transformer.transformer.layers[6].self_attn.weights_per_step\n",
      "encoder_transformer.transformer.layers[6].self_attn.in_proj.weight\n",
      "encoder_transformer.transformer.layers[6].self_attn.out_proj.weight\n",
      "encoder_transformer.transformer.layers[6].norm1.weight\n",
      "encoder_transformer.transformer.layers[6].norm1.bias\n",
      "encoder_transformer.transformer.layers[6].norm2.weight\n",
      "encoder_transformer.transformer.layers[6].norm2.bias\n",
      "encoder_transformer.transformer.layers[6].skip_self_attn\n",
      "encoder_transformer.transformer.layers[6].activation\n",
      "encoder_transformer.transformer.layers[6].layer_scale_1.scale\n",
      "encoder_transformer.transformer.layers[6].layer_scale_1.channel_last\n",
      "encoder_transformer.transformer.layers[6].layer_scale_2.scale\n",
      "encoder_transformer.transformer.layers[6].layer_scale_2.channel_last\n",
      "encoder_transformer.transformer.layers[6].weights_per_step\n",
      "encoder_transformer.transformer.layers[7].linear1.weight\n",
      "encoder_transformer.transformer.layers[7].linear2.weight\n",
      "encoder_transformer.transformer.layers[7].self_attn.embed_dim\n",
      "encoder_transformer.transformer.layers[7].self_attn.causal\n",
      "encoder_transformer.transformer.layers[7].self_attn.context\n",
      "encoder_transformer.transformer.layers[7].self_attn.rope.max_period\n",
      "encoder_transformer.transformer.layers[7].self_attn.num_heads\n",
      "encoder_transformer.transformer.layers[7].self_attn.weights_per_step\n",
      "encoder_transformer.transformer.layers[7].self_attn.in_proj.weight\n",
      "encoder_transformer.transformer.layers[7].self_attn.out_proj.weight\n",
      "encoder_transformer.transformer.layers[7].norm1.weight\n",
      "encoder_transformer.transformer.layers[7].norm1.bias\n",
      "encoder_transformer.transformer.layers[7].norm2.weight\n",
      "encoder_transformer.transformer.layers[7].norm2.bias\n",
      "encoder_transformer.transformer.layers[7].skip_self_attn\n",
      "encoder_transformer.transformer.layers[7].activation\n",
      "encoder_transformer.transformer.layers[7].layer_scale_1.scale\n",
      "encoder_transformer.transformer.layers[7].layer_scale_1.channel_last\n",
      "encoder_transformer.transformer.layers[7].layer_scale_2.scale\n",
      "encoder_transformer.transformer.layers[7].layer_scale_2.channel_last\n",
      "encoder_transformer.transformer.layers[7].weights_per_step\n",
      "encoder_transformer.input_dimension\n",
      "encoder_transformer.output_dimensions[0]\n",
      "encoder_transformer.conv_layout\n",
      "quantizer.max_n_q\n",
      "quantizer.n_q_semantic\n",
      "quantizer.n_q_acoustic\n",
      "quantizer.rvq_first.max_n_q\n",
      "quantizer.rvq_first.n_q\n",
      "quantizer.rvq_first.q_dropout\n",
      "quantizer.rvq_first.dropout.p\n",
      "quantizer.rvq_first.dropout.inference\n",
      "quantizer.rvq_first.dimension\n",
      "quantizer.rvq_first.input_dimension\n",
      "quantizer.rvq_first.output_dimension\n",
      "quantizer.rvq_first.bins\n",
      "quantizer.rvq_first.decay\n",
      "quantizer.rvq_first.input_proj.weight\n",
      "quantizer.rvq_first.output_proj.weight\n",
      "quantizer.rvq_first.vq.layers[0].epsilon\n",
      "quantizer.rvq_first.vq.layers[0]._codebook.embedding\n",
      "quantizer.rvq_first.vq.layers[0]._codebook.dim\n",
      "quantizer.rvq_first.vq.layers[0]._codebook.codebook_size\n",
      "quantizer.rvq_first.vq.layers[0]._codebook.decay\n",
      "quantizer.rvq_first.vq.layers[0]._codebook.epsilon\n",
      "quantizer.rvq_first.vq.layers[0]._codebook.threshold_usage_ratio\n",
      "quantizer.rvq_first.vq.layers[0]._codebook.replaced_usage_ratio\n",
      "quantizer.rvq_first.vq.layers[0]._codebook.check_unused_every\n",
      "quantizer.rvq_first.vq.layers[0]._codebook._cached_initialized\n",
      "quantizer.rvq_first.vq.layers[0]._codebook._next_unused_check\n",
      "quantizer.rvq_first.vq.layers[0].codebook_size\n",
      "quantizer.rvq_first.vq.codebook_offset\n",
      "quantizer.rvq_rest.max_n_q\n",
      "quantizer.rvq_rest.n_q\n",
      "quantizer.rvq_rest.q_dropout\n",
      "quantizer.rvq_rest.dropout.p\n",
      "quantizer.rvq_rest.dropout.inference\n",
      "quantizer.rvq_rest.dimension\n",
      "quantizer.rvq_rest.input_dimension\n",
      "quantizer.rvq_rest.output_dimension\n",
      "quantizer.rvq_rest.bins\n",
      "quantizer.rvq_rest.decay\n",
      "quantizer.rvq_rest.input_proj.weight\n",
      "quantizer.rvq_rest.output_proj.weight\n",
      "quantizer.rvq_rest.vq.layers[0].epsilon\n",
      "quantizer.rvq_rest.vq.layers[0]._codebook.embedding\n",
      "quantizer.rvq_rest.vq.layers[0]._codebook.dim\n",
      "quantizer.rvq_rest.vq.layers[0]._codebook.codebook_size\n",
      "quantizer.rvq_rest.vq.layers[0]._codebook.decay\n",
      "quantizer.rvq_rest.vq.layers[0]._codebook.epsilon\n",
      "quantizer.rvq_rest.vq.layers[0]._codebook.threshold_usage_ratio\n",
      "quantizer.rvq_rest.vq.layers[0]._codebook.replaced_usage_ratio\n",
      "quantizer.rvq_rest.vq.layers[0]._codebook.check_unused_every\n",
      "quantizer.rvq_rest.vq.layers[0]._codebook._cached_initialized\n",
      "quantizer.rvq_rest.vq.layers[0]._codebook._next_unused_check\n",
      "quantizer.rvq_rest.vq.layers[0].codebook_size\n",
      "quantizer.rvq_rest.vq.layers[1].epsilon\n",
      "quantizer.rvq_rest.vq.layers[1]._codebook.embedding\n",
      "quantizer.rvq_rest.vq.layers[1]._codebook.dim\n",
      "quantizer.rvq_rest.vq.layers[1]._codebook.codebook_size\n",
      "quantizer.rvq_rest.vq.layers[1]._codebook.decay\n",
      "quantizer.rvq_rest.vq.layers[1]._codebook.epsilon\n",
      "quantizer.rvq_rest.vq.layers[1]._codebook.threshold_usage_ratio\n",
      "quantizer.rvq_rest.vq.layers[1]._codebook.replaced_usage_ratio\n",
      "quantizer.rvq_rest.vq.layers[1]._codebook.check_unused_every\n",
      "quantizer.rvq_rest.vq.layers[1]._codebook._cached_initialized\n",
      "quantizer.rvq_rest.vq.layers[1]._codebook._next_unused_check\n",
      "quantizer.rvq_rest.vq.layers[1].codebook_size\n",
      "quantizer.rvq_rest.vq.layers[2].epsilon\n",
      "quantizer.rvq_rest.vq.layers[2]._codebook.embedding\n",
      "quantizer.rvq_rest.vq.layers[2]._codebook.dim\n",
      "quantizer.rvq_rest.vq.layers[2]._codebook.codebook_size\n",
      "quantizer.rvq_rest.vq.layers[2]._codebook.decay\n",
      "quantizer.rvq_rest.vq.layers[2]._codebook.epsilon\n",
      "quantizer.rvq_rest.vq.layers[2]._codebook.threshold_usage_ratio\n",
      "quantizer.rvq_rest.vq.layers[2]._codebook.replaced_usage_ratio\n",
      "quantizer.rvq_rest.vq.layers[2]._codebook.check_unused_every\n",
      "quantizer.rvq_rest.vq.layers[2]._codebook._cached_initialized\n",
      "quantizer.rvq_rest.vq.layers[2]._codebook._next_unused_check\n",
      "quantizer.rvq_rest.vq.layers[2].codebook_size\n",
      "quantizer.rvq_rest.vq.layers[3].epsilon\n",
      "quantizer.rvq_rest.vq.layers[3]._codebook.embedding\n",
      "quantizer.rvq_rest.vq.layers[3]._codebook.dim\n",
      "quantizer.rvq_rest.vq.layers[3]._codebook.codebook_size\n",
      "quantizer.rvq_rest.vq.layers[3]._codebook.decay\n",
      "quantizer.rvq_rest.vq.layers[3]._codebook.epsilon\n",
      "quantizer.rvq_rest.vq.layers[3]._codebook.threshold_usage_ratio\n",
      "quantizer.rvq_rest.vq.layers[3]._codebook.replaced_usage_ratio\n",
      "quantizer.rvq_rest.vq.layers[3]._codebook.check_unused_every\n",
      "quantizer.rvq_rest.vq.layers[3]._codebook._cached_initialized\n",
      "quantizer.rvq_rest.vq.layers[3]._codebook._next_unused_check\n",
      "quantizer.rvq_rest.vq.layers[3].codebook_size\n",
      "quantizer.rvq_rest.vq.layers[4].epsilon\n",
      "quantizer.rvq_rest.vq.layers[4]._codebook.embedding\n",
      "quantizer.rvq_rest.vq.layers[4]._codebook.dim\n",
      "quantizer.rvq_rest.vq.layers[4]._codebook.codebook_size\n",
      "quantizer.rvq_rest.vq.layers[4]._codebook.decay\n",
      "quantizer.rvq_rest.vq.layers[4]._codebook.epsilon\n",
      "quantizer.rvq_rest.vq.layers[4]._codebook.threshold_usage_ratio\n",
      "quantizer.rvq_rest.vq.layers[4]._codebook.replaced_usage_ratio\n",
      "quantizer.rvq_rest.vq.layers[4]._codebook.check_unused_every\n",
      "quantizer.rvq_rest.vq.layers[4]._codebook._cached_initialized\n",
      "quantizer.rvq_rest.vq.layers[4]._codebook._next_unused_check\n",
      "quantizer.rvq_rest.vq.layers[4].codebook_size\n",
      "quantizer.rvq_rest.vq.layers[5].epsilon\n",
      "quantizer.rvq_rest.vq.layers[5]._codebook.embedding\n",
      "quantizer.rvq_rest.vq.layers[5]._codebook.dim\n",
      "quantizer.rvq_rest.vq.layers[5]._codebook.codebook_size\n",
      "quantizer.rvq_rest.vq.layers[5]._codebook.decay\n",
      "quantizer.rvq_rest.vq.layers[5]._codebook.epsilon\n",
      "quantizer.rvq_rest.vq.layers[5]._codebook.threshold_usage_ratio\n",
      "quantizer.rvq_rest.vq.layers[5]._codebook.replaced_usage_ratio\n",
      "quantizer.rvq_rest.vq.layers[5]._codebook.check_unused_every\n",
      "quantizer.rvq_rest.vq.layers[5]._codebook._cached_initialized\n",
      "quantizer.rvq_rest.vq.layers[5]._codebook._next_unused_check\n",
      "quantizer.rvq_rest.vq.layers[5].codebook_size\n",
      "quantizer.rvq_rest.vq.layers[6].epsilon\n",
      "quantizer.rvq_rest.vq.layers[6]._codebook.embedding\n",
      "quantizer.rvq_rest.vq.layers[6]._codebook.dim\n",
      "quantizer.rvq_rest.vq.layers[6]._codebook.codebook_size\n",
      "quantizer.rvq_rest.vq.layers[6]._codebook.decay\n",
      "quantizer.rvq_rest.vq.layers[6]._codebook.epsilon\n",
      "quantizer.rvq_rest.vq.layers[6]._codebook.threshold_usage_ratio\n",
      "quantizer.rvq_rest.vq.layers[6]._codebook.replaced_usage_ratio\n",
      "quantizer.rvq_rest.vq.layers[6]._codebook.check_unused_every\n",
      "quantizer.rvq_rest.vq.layers[6]._codebook._cached_initialized\n",
      "quantizer.rvq_rest.vq.layers[6]._codebook._next_unused_check\n",
      "quantizer.rvq_rest.vq.layers[6].codebook_size\n",
      "quantizer.rvq_rest.vq.codebook_offset\n",
      "_frame_rate\n",
      "_sample_rate\n",
      "_channels\n",
      "encoder_frame_rate\n",
      "downsample.learnt\n",
      "downsample.channel_wise\n",
      "downsample.conv.conv.conv.weight\n",
      "downsample.conv.conv.norm_type\n",
      "downsample.conv.causal\n",
      "downsample.conv.pad_mode\n",
      "upsample.learnt\n",
      "upsample.channel_wise\n",
      "upsample.convtr.convtr.convtr.weight\n",
      "upsample.convtr.convtr.norm_type\n",
      "upsample.convtr.causal\n",
      "upsample.convtr.trim_right_ratio\n",
      "dimension\n",
      "resample_method\n",
      "rvq_first.vq.layers[0]._codebook.embedding\n",
      "rvq_rest.vq.layers[0]._codebook.embedding\n",
      "rvq_rest.vq.layers[1]._codebook.embedding\n",
      "rvq_rest.vq.layers[2]._codebook.embedding\n",
      "rvq_rest.vq.layers[3]._codebook.embedding\n",
      "rvq_rest.vq.layers[4]._codebook.embedding\n",
      "rvq_rest.vq.layers[5]._codebook.embedding\n",
      "rvq_rest.vq.layers[6]._codebook.embedding\n",
      ".decoder.channels\n",
      ".decoder.dimension\n",
      ".decoder.n_filters\n",
      ".decoder.ratios[0]\n",
      ".decoder.ratios[1]\n",
      ".decoder.ratios[2]\n",
      ".decoder.ratios[3]\n",
      ".decoder.n_residual_layers\n",
      ".decoder.hop_length\n",
      ".decoder.n_blocks\n",
      ".decoder.disable_norm_outer_blocks\n",
      ".decoder.act\n",
      ".decoder.first_layer.conv.conv.weight\n",
      "first_layer.conv.conv.weight\n",
      ".decoder.first_layer.conv.conv.bias\n",
      "first_layer.conv.conv.bias\n",
      ".decoder.first_layer.conv.norm_type\n",
      ".decoder.first_layer.causal\n",
      ".decoder.first_layer.pad_mode\n",
      ".decoder.blocks[0][0].convtr.convtr.weight\n",
      "blocks[0][0].convtr.convtr.weight\n",
      ".decoder.blocks[0][0].convtr.convtr.bias\n",
      "blocks[0][0].convtr.convtr.bias\n",
      ".decoder.blocks[0][0].convtr.norm_type\n",
      ".decoder.blocks[0][0].causal\n",
      ".decoder.blocks[0][0].trim_right_ratio\n",
      ".decoder.blocks[0][1][0].act\n",
      ".decoder.blocks[0][1][0].blocks[0].conv.conv.weight\n",
      "blocks[0][1][0].blocks[0].conv.conv.weight\n",
      ".decoder.blocks[0][1][0].blocks[0].conv.conv.bias\n",
      "blocks[0][1][0].blocks[0].conv.conv.bias\n",
      ".decoder.blocks[0][1][0].blocks[0].conv.norm_type\n",
      ".decoder.blocks[0][1][0].blocks[0].causal\n",
      ".decoder.blocks[0][1][0].blocks[0].pad_mode\n",
      ".decoder.blocks[0][1][0].blocks[1].conv.conv.weight\n",
      "blocks[0][1][0].blocks[1].conv.conv.weight\n",
      ".decoder.blocks[0][1][0].blocks[1].conv.conv.bias\n",
      "blocks[0][1][0].blocks[1].conv.conv.bias\n",
      ".decoder.blocks[0][1][0].blocks[1].conv.norm_type\n",
      ".decoder.blocks[0][1][0].blocks[1].causal\n",
      ".decoder.blocks[0][1][0].blocks[1].pad_mode\n",
      ".decoder.blocks[0][1][0].true_skip\n",
      ".decoder.blocks[1][0].convtr.convtr.weight\n",
      "blocks[1][0].convtr.convtr.weight\n",
      ".decoder.blocks[1][0].convtr.convtr.bias\n",
      "blocks[1][0].convtr.convtr.bias\n",
      ".decoder.blocks[1][0].convtr.norm_type\n",
      ".decoder.blocks[1][0].causal\n",
      ".decoder.blocks[1][0].trim_right_ratio\n",
      ".decoder.blocks[1][1][0].act\n",
      ".decoder.blocks[1][1][0].blocks[0].conv.conv.weight\n",
      "blocks[1][1][0].blocks[0].conv.conv.weight\n",
      ".decoder.blocks[1][1][0].blocks[0].conv.conv.bias\n",
      "blocks[1][1][0].blocks[0].conv.conv.bias\n",
      ".decoder.blocks[1][1][0].blocks[0].conv.norm_type\n",
      ".decoder.blocks[1][1][0].blocks[0].causal\n",
      ".decoder.blocks[1][1][0].blocks[0].pad_mode\n",
      ".decoder.blocks[1][1][0].blocks[1].conv.conv.weight\n",
      "blocks[1][1][0].blocks[1].conv.conv.weight\n",
      ".decoder.blocks[1][1][0].blocks[1].conv.conv.bias\n",
      "blocks[1][1][0].blocks[1].conv.conv.bias\n",
      ".decoder.blocks[1][1][0].blocks[1].conv.norm_type\n",
      ".decoder.blocks[1][1][0].blocks[1].causal\n",
      ".decoder.blocks[1][1][0].blocks[1].pad_mode\n",
      ".decoder.blocks[1][1][0].true_skip\n",
      ".decoder.blocks[2][0].convtr.convtr.weight\n",
      "blocks[2][0].convtr.convtr.weight\n",
      ".decoder.blocks[2][0].convtr.convtr.bias\n",
      "blocks[2][0].convtr.convtr.bias\n",
      ".decoder.blocks[2][0].convtr.norm_type\n",
      ".decoder.blocks[2][0].causal\n",
      ".decoder.blocks[2][0].trim_right_ratio\n",
      ".decoder.blocks[2][1][0].act\n",
      ".decoder.blocks[2][1][0].blocks[0].conv.conv.weight\n",
      "blocks[2][1][0].blocks[0].conv.conv.weight\n",
      ".decoder.blocks[2][1][0].blocks[0].conv.conv.bias\n",
      "blocks[2][1][0].blocks[0].conv.conv.bias\n",
      ".decoder.blocks[2][1][0].blocks[0].conv.norm_type\n",
      ".decoder.blocks[2][1][0].blocks[0].causal\n",
      ".decoder.blocks[2][1][0].blocks[0].pad_mode\n",
      ".decoder.blocks[2][1][0].blocks[1].conv.conv.weight\n",
      "blocks[2][1][0].blocks[1].conv.conv.weight\n",
      ".decoder.blocks[2][1][0].blocks[1].conv.conv.bias\n",
      "blocks[2][1][0].blocks[1].conv.conv.bias\n",
      ".decoder.blocks[2][1][0].blocks[1].conv.norm_type\n",
      ".decoder.blocks[2][1][0].blocks[1].causal\n",
      ".decoder.blocks[2][1][0].blocks[1].pad_mode\n",
      ".decoder.blocks[2][1][0].true_skip\n",
      ".decoder.blocks[3][0].convtr.convtr.weight\n",
      "blocks[3][0].convtr.convtr.weight\n",
      ".decoder.blocks[3][0].convtr.convtr.bias\n",
      "blocks[3][0].convtr.convtr.bias\n",
      ".decoder.blocks[3][0].convtr.norm_type\n",
      ".decoder.blocks[3][0].causal\n",
      ".decoder.blocks[3][0].trim_right_ratio\n",
      ".decoder.blocks[3][1][0].act\n",
      ".decoder.blocks[3][1][0].blocks[0].conv.conv.weight\n",
      "blocks[3][1][0].blocks[0].conv.conv.weight\n",
      ".decoder.blocks[3][1][0].blocks[0].conv.conv.bias\n",
      "blocks[3][1][0].blocks[0].conv.conv.bias\n",
      ".decoder.blocks[3][1][0].blocks[0].conv.norm_type\n",
      ".decoder.blocks[3][1][0].blocks[0].causal\n",
      ".decoder.blocks[3][1][0].blocks[0].pad_mode\n",
      ".decoder.blocks[3][1][0].blocks[1].conv.conv.weight\n",
      "blocks[3][1][0].blocks[1].conv.conv.weight\n",
      ".decoder.blocks[3][1][0].blocks[1].conv.conv.bias\n",
      "blocks[3][1][0].blocks[1].conv.conv.bias\n",
      ".decoder.blocks[3][1][0].blocks[1].conv.norm_type\n",
      ".decoder.blocks[3][1][0].blocks[1].causal\n",
      ".decoder.blocks[3][1][0].blocks[1].pad_mode\n",
      ".decoder.blocks[3][1][0].true_skip\n",
      ".decoder.last_layer.conv.conv.weight\n",
      "last_layer.conv.conv.weight\n",
      ".decoder.last_layer.conv.conv.bias\n",
      "last_layer.conv.conv.bias\n",
      ".decoder.last_layer.conv.norm_type\n",
      ".decoder.last_layer.causal\n",
      ".decoder.last_layer.pad_mode\n",
      ".encoder.channels\n",
      ".encoder.dimension\n",
      ".encoder.n_filters\n",
      ".encoder.ratios[0]\n",
      ".encoder.ratios[1]\n",
      ".encoder.ratios[2]\n",
      ".encoder.ratios[3]\n",
      ".encoder.n_residual_layers\n",
      ".encoder.hop_length\n",
      ".encoder.n_blocks\n",
      ".encoder.disable_norm_outer_blocks\n",
      ".encoder.act\n",
      ".encoder.first_layer.conv.conv.weight\n",
      ".encoder.first_layer.conv.conv.bias\n",
      ".encoder.first_layer.conv.norm_type\n",
      ".encoder.first_layer.causal\n",
      ".encoder.first_layer.pad_mode\n",
      ".encoder.blocks[0][0][0].act\n",
      ".encoder.blocks[0][0][0].blocks[0].conv.conv.weight\n",
      ".encoder.blocks[0][0][0].blocks[0].conv.conv.bias\n",
      ".encoder.blocks[0][0][0].blocks[0].conv.norm_type\n",
      ".encoder.blocks[0][0][0].blocks[0].causal\n",
      ".encoder.blocks[0][0][0].blocks[0].pad_mode\n",
      ".encoder.blocks[0][0][0].blocks[1].conv.conv.weight\n",
      ".encoder.blocks[0][0][0].blocks[1].conv.conv.bias\n",
      ".encoder.blocks[0][0][0].blocks[1].conv.norm_type\n",
      ".encoder.blocks[0][0][0].blocks[1].causal\n",
      ".encoder.blocks[0][0][0].blocks[1].pad_mode\n",
      ".encoder.blocks[0][0][0].true_skip\n",
      ".encoder.blocks[0][1].conv.conv.weight\n",
      ".encoder.blocks[0][1].conv.conv.bias\n",
      ".encoder.blocks[0][1].conv.norm_type\n",
      ".encoder.blocks[0][1].causal\n",
      ".encoder.blocks[0][1].pad_mode\n",
      ".encoder.blocks[1][0][0].act\n",
      ".encoder.blocks[1][0][0].blocks[0].conv.conv.weight\n",
      ".encoder.blocks[1][0][0].blocks[0].conv.conv.bias\n",
      ".encoder.blocks[1][0][0].blocks[0].conv.norm_type\n",
      ".encoder.blocks[1][0][0].blocks[0].causal\n",
      ".encoder.blocks[1][0][0].blocks[0].pad_mode\n",
      ".encoder.blocks[1][0][0].blocks[1].conv.conv.weight\n",
      ".encoder.blocks[1][0][0].blocks[1].conv.conv.bias\n",
      ".encoder.blocks[1][0][0].blocks[1].conv.norm_type\n",
      ".encoder.blocks[1][0][0].blocks[1].causal\n",
      ".encoder.blocks[1][0][0].blocks[1].pad_mode\n",
      ".encoder.blocks[1][0][0].true_skip\n",
      ".encoder.blocks[1][1].conv.conv.weight\n",
      ".encoder.blocks[1][1].conv.conv.bias\n",
      ".encoder.blocks[1][1].conv.norm_type\n",
      ".encoder.blocks[1][1].causal\n",
      ".encoder.blocks[1][1].pad_mode\n",
      ".encoder.blocks[2][0][0].act\n",
      ".encoder.blocks[2][0][0].blocks[0].conv.conv.weight\n",
      ".encoder.blocks[2][0][0].blocks[0].conv.conv.bias\n",
      ".encoder.blocks[2][0][0].blocks[0].conv.norm_type\n",
      ".encoder.blocks[2][0][0].blocks[0].causal\n",
      ".encoder.blocks[2][0][0].blocks[0].pad_mode\n",
      ".encoder.blocks[2][0][0].blocks[1].conv.conv.weight\n",
      ".encoder.blocks[2][0][0].blocks[1].conv.conv.bias\n",
      ".encoder.blocks[2][0][0].blocks[1].conv.norm_type\n",
      ".encoder.blocks[2][0][0].blocks[1].causal\n",
      ".encoder.blocks[2][0][0].blocks[1].pad_mode\n",
      ".encoder.blocks[2][0][0].true_skip\n",
      ".encoder.blocks[2][1].conv.conv.weight\n",
      ".encoder.blocks[2][1].conv.conv.bias\n",
      ".encoder.blocks[2][1].conv.norm_type\n",
      ".encoder.blocks[2][1].causal\n",
      ".encoder.blocks[2][1].pad_mode\n",
      ".encoder.blocks[3][0][0].act\n",
      ".encoder.blocks[3][0][0].blocks[0].conv.conv.weight\n",
      ".encoder.blocks[3][0][0].blocks[0].conv.conv.bias\n",
      ".encoder.blocks[3][0][0].blocks[0].conv.norm_type\n",
      ".encoder.blocks[3][0][0].blocks[0].causal\n",
      ".encoder.blocks[3][0][0].blocks[0].pad_mode\n",
      ".encoder.blocks[3][0][0].blocks[1].conv.conv.weight\n",
      ".encoder.blocks[3][0][0].blocks[1].conv.conv.bias\n",
      ".encoder.blocks[3][0][0].blocks[1].conv.norm_type\n",
      ".encoder.blocks[3][0][0].blocks[1].causal\n",
      ".encoder.blocks[3][0][0].blocks[1].pad_mode\n",
      ".encoder.blocks[3][0][0].true_skip\n",
      ".encoder.blocks[3][1].conv.conv.weight\n",
      ".encoder.blocks[3][1].conv.conv.bias\n",
      ".encoder.blocks[3][1].conv.norm_type\n",
      ".encoder.blocks[3][1].causal\n",
      ".encoder.blocks[3][1].pad_mode\n",
      ".encoder.last_layer.conv.conv.weight\n",
      ".encoder.last_layer.conv.conv.bias\n",
      ".encoder.last_layer.conv.norm_type\n",
      ".encoder.last_layer.causal\n",
      ".encoder.last_layer.pad_mode\n",
      ".decoder.channels\n",
      ".decoder.dimension\n",
      ".decoder.n_filters\n",
      ".decoder.ratios[0]\n",
      ".decoder.ratios[1]\n",
      ".decoder.ratios[2]\n",
      ".decoder.ratios[3]\n",
      ".decoder.n_residual_layers\n",
      ".decoder.hop_length\n",
      ".decoder.n_blocks\n",
      ".decoder.disable_norm_outer_blocks\n",
      ".decoder.act\n",
      ".decoder.first_layer.conv.conv.weight\n",
      ".decoder.first_layer.conv.conv.bias\n",
      ".decoder.first_layer.conv.norm_type\n",
      ".decoder.first_layer.causal\n",
      ".decoder.first_layer.pad_mode\n",
      ".decoder.blocks[0][0].convtr.convtr.weight\n",
      ".decoder.blocks[0][0].convtr.convtr.bias\n",
      ".decoder.blocks[0][0].convtr.norm_type\n",
      ".decoder.blocks[0][0].causal\n",
      ".decoder.blocks[0][0].trim_right_ratio\n",
      ".decoder.blocks[0][1][0].act\n",
      ".decoder.blocks[0][1][0].blocks[0].conv.conv.weight\n",
      ".decoder.blocks[0][1][0].blocks[0].conv.conv.bias\n",
      ".decoder.blocks[0][1][0].blocks[0].conv.norm_type\n",
      ".decoder.blocks[0][1][0].blocks[0].causal\n",
      ".decoder.blocks[0][1][0].blocks[0].pad_mode\n",
      ".decoder.blocks[0][1][0].blocks[1].conv.conv.weight\n",
      ".decoder.blocks[0][1][0].blocks[1].conv.conv.bias\n",
      ".decoder.blocks[0][1][0].blocks[1].conv.norm_type\n",
      ".decoder.blocks[0][1][0].blocks[1].causal\n",
      ".decoder.blocks[0][1][0].blocks[1].pad_mode\n",
      ".decoder.blocks[0][1][0].true_skip\n",
      ".decoder.blocks[1][0].convtr.convtr.weight\n",
      ".decoder.blocks[1][0].convtr.convtr.bias\n",
      ".decoder.blocks[1][0].convtr.norm_type\n",
      ".decoder.blocks[1][0].causal\n",
      ".decoder.blocks[1][0].trim_right_ratio\n",
      ".decoder.blocks[1][1][0].act\n",
      ".decoder.blocks[1][1][0].blocks[0].conv.conv.weight\n",
      ".decoder.blocks[1][1][0].blocks[0].conv.conv.bias\n",
      ".decoder.blocks[1][1][0].blocks[0].conv.norm_type\n",
      ".decoder.blocks[1][1][0].blocks[0].causal\n",
      ".decoder.blocks[1][1][0].blocks[0].pad_mode\n",
      ".decoder.blocks[1][1][0].blocks[1].conv.conv.weight\n",
      ".decoder.blocks[1][1][0].blocks[1].conv.conv.bias\n",
      ".decoder.blocks[1][1][0].blocks[1].conv.norm_type\n",
      ".decoder.blocks[1][1][0].blocks[1].causal\n",
      ".decoder.blocks[1][1][0].blocks[1].pad_mode\n",
      ".decoder.blocks[1][1][0].true_skip\n",
      ".decoder.blocks[2][0].convtr.convtr.weight\n",
      ".decoder.blocks[2][0].convtr.convtr.bias\n",
      ".decoder.blocks[2][0].convtr.norm_type\n",
      ".decoder.blocks[2][0].causal\n",
      ".decoder.blocks[2][0].trim_right_ratio\n",
      ".decoder.blocks[2][1][0].act\n",
      ".decoder.blocks[2][1][0].blocks[0].conv.conv.weight\n",
      ".decoder.blocks[2][1][0].blocks[0].conv.conv.bias\n",
      ".decoder.blocks[2][1][0].blocks[0].conv.norm_type\n",
      ".decoder.blocks[2][1][0].blocks[0].causal\n",
      ".decoder.blocks[2][1][0].blocks[0].pad_mode\n",
      ".decoder.blocks[2][1][0].blocks[1].conv.conv.weight\n",
      ".decoder.blocks[2][1][0].blocks[1].conv.conv.bias\n",
      ".decoder.blocks[2][1][0].blocks[1].conv.norm_type\n",
      ".decoder.blocks[2][1][0].blocks[1].causal\n",
      ".decoder.blocks[2][1][0].blocks[1].pad_mode\n",
      ".decoder.blocks[2][1][0].true_skip\n",
      ".decoder.blocks[3][0].convtr.convtr.weight\n",
      ".decoder.blocks[3][0].convtr.convtr.bias\n",
      ".decoder.blocks[3][0].convtr.norm_type\n",
      ".decoder.blocks[3][0].causal\n",
      ".decoder.blocks[3][0].trim_right_ratio\n",
      ".decoder.blocks[3][1][0].act\n",
      ".decoder.blocks[3][1][0].blocks[0].conv.conv.weight\n",
      ".decoder.blocks[3][1][0].blocks[0].conv.conv.bias\n",
      ".decoder.blocks[3][1][0].blocks[0].conv.norm_type\n",
      ".decoder.blocks[3][1][0].blocks[0].causal\n",
      ".decoder.blocks[3][1][0].blocks[0].pad_mode\n",
      ".decoder.blocks[3][1][0].blocks[1].conv.conv.weight\n",
      ".decoder.blocks[3][1][0].blocks[1].conv.conv.bias\n",
      ".decoder.blocks[3][1][0].blocks[1].conv.norm_type\n",
      ".decoder.blocks[3][1][0].blocks[1].causal\n",
      ".decoder.blocks[3][1][0].blocks[1].pad_mode\n",
      ".decoder.blocks[3][1][0].true_skip\n",
      ".decoder.last_layer.conv.conv.weight\n",
      ".decoder.last_layer.conv.conv.bias\n",
      ".decoder.last_layer.conv.norm_type\n",
      ".decoder.last_layer.causal\n",
      ".decoder.last_layer.pad_mode\n",
      ".decoder_transformer.transformer.positional_embedding\n",
      ".decoder_transformer.transformer.max_period\n",
      ".decoder_transformer.transformer.positional_scale\n",
      ".decoder_transformer.transformer.rope.max_period\n",
      ".decoder_transformer.transformer.layers[0].linear1.weight\n",
      ".decoder_transformer.transformer.layers[0].linear2.weight\n",
      ".decoder_transformer.transformer.layers[0].self_attn.embed_dim\n",
      ".decoder_transformer.transformer.layers[0].self_attn.causal\n",
      ".decoder_transformer.transformer.layers[0].self_attn.context\n",
      ".decoder_transformer.transformer.layers[0].self_attn.rope.max_period\n",
      ".decoder_transformer.transformer.layers[0].self_attn.num_heads\n",
      ".decoder_transformer.transformer.layers[0].self_attn.weights_per_step\n",
      ".decoder_transformer.transformer.layers[0].self_attn.in_proj.weight\n",
      ".decoder_transformer.transformer.layers[0].self_attn.out_proj.weight\n",
      ".decoder_transformer.transformer.layers[0].norm1.weight\n",
      ".decoder_transformer.transformer.layers[0].norm1.bias\n",
      ".decoder_transformer.transformer.layers[0].norm2.weight\n",
      ".decoder_transformer.transformer.layers[0].norm2.bias\n",
      ".decoder_transformer.transformer.layers[0].skip_self_attn\n",
      ".decoder_transformer.transformer.layers[0].activation\n",
      ".decoder_transformer.transformer.layers[0].layer_scale_1.scale\n",
      ".decoder_transformer.transformer.layers[0].layer_scale_1.channel_last\n",
      ".decoder_transformer.transformer.layers[0].layer_scale_2.scale\n",
      ".decoder_transformer.transformer.layers[0].layer_scale_2.channel_last\n",
      ".decoder_transformer.transformer.layers[0].weights_per_step\n",
      ".decoder_transformer.transformer.layers[1].linear1.weight\n",
      ".decoder_transformer.transformer.layers[1].linear2.weight\n",
      ".decoder_transformer.transformer.layers[1].self_attn.embed_dim\n",
      ".decoder_transformer.transformer.layers[1].self_attn.causal\n",
      ".decoder_transformer.transformer.layers[1].self_attn.context\n",
      ".decoder_transformer.transformer.layers[1].self_attn.rope.max_period\n",
      ".decoder_transformer.transformer.layers[1].self_attn.num_heads\n",
      ".decoder_transformer.transformer.layers[1].self_attn.weights_per_step\n",
      ".decoder_transformer.transformer.layers[1].self_attn.in_proj.weight\n",
      ".decoder_transformer.transformer.layers[1].self_attn.out_proj.weight\n",
      ".decoder_transformer.transformer.layers[1].norm1.weight\n",
      ".decoder_transformer.transformer.layers[1].norm1.bias\n",
      ".decoder_transformer.transformer.layers[1].norm2.weight\n",
      ".decoder_transformer.transformer.layers[1].norm2.bias\n",
      ".decoder_transformer.transformer.layers[1].skip_self_attn\n",
      ".decoder_transformer.transformer.layers[1].activation\n",
      ".decoder_transformer.transformer.layers[1].layer_scale_1.scale\n",
      ".decoder_transformer.transformer.layers[1].layer_scale_1.channel_last\n",
      ".decoder_transformer.transformer.layers[1].layer_scale_2.scale\n",
      ".decoder_transformer.transformer.layers[1].layer_scale_2.channel_last\n",
      ".decoder_transformer.transformer.layers[1].weights_per_step\n",
      ".decoder_transformer.transformer.layers[2].linear1.weight\n",
      ".decoder_transformer.transformer.layers[2].linear2.weight\n",
      ".decoder_transformer.transformer.layers[2].self_attn.embed_dim\n",
      ".decoder_transformer.transformer.layers[2].self_attn.causal\n",
      ".decoder_transformer.transformer.layers[2].self_attn.context\n",
      ".decoder_transformer.transformer.layers[2].self_attn.rope.max_period\n",
      ".decoder_transformer.transformer.layers[2].self_attn.num_heads\n",
      ".decoder_transformer.transformer.layers[2].self_attn.weights_per_step\n",
      ".decoder_transformer.transformer.layers[2].self_attn.in_proj.weight\n",
      ".decoder_transformer.transformer.layers[2].self_attn.out_proj.weight\n",
      ".decoder_transformer.transformer.layers[2].norm1.weight\n",
      ".decoder_transformer.transformer.layers[2].norm1.bias\n",
      ".decoder_transformer.transformer.layers[2].norm2.weight\n",
      ".decoder_transformer.transformer.layers[2].norm2.bias\n",
      ".decoder_transformer.transformer.layers[2].skip_self_attn\n",
      ".decoder_transformer.transformer.layers[2].activation\n",
      ".decoder_transformer.transformer.layers[2].layer_scale_1.scale\n",
      ".decoder_transformer.transformer.layers[2].layer_scale_1.channel_last\n",
      ".decoder_transformer.transformer.layers[2].layer_scale_2.scale\n",
      ".decoder_transformer.transformer.layers[2].layer_scale_2.channel_last\n",
      ".decoder_transformer.transformer.layers[2].weights_per_step\n",
      ".decoder_transformer.transformer.layers[3].linear1.weight\n",
      ".decoder_transformer.transformer.layers[3].linear2.weight\n",
      ".decoder_transformer.transformer.layers[3].self_attn.embed_dim\n",
      ".decoder_transformer.transformer.layers[3].self_attn.causal\n",
      ".decoder_transformer.transformer.layers[3].self_attn.context\n",
      ".decoder_transformer.transformer.layers[3].self_attn.rope.max_period\n",
      ".decoder_transformer.transformer.layers[3].self_attn.num_heads\n",
      ".decoder_transformer.transformer.layers[3].self_attn.weights_per_step\n",
      ".decoder_transformer.transformer.layers[3].self_attn.in_proj.weight\n",
      ".decoder_transformer.transformer.layers[3].self_attn.out_proj.weight\n",
      ".decoder_transformer.transformer.layers[3].norm1.weight\n",
      ".decoder_transformer.transformer.layers[3].norm1.bias\n",
      ".decoder_transformer.transformer.layers[3].norm2.weight\n",
      ".decoder_transformer.transformer.layers[3].norm2.bias\n",
      ".decoder_transformer.transformer.layers[3].skip_self_attn\n",
      ".decoder_transformer.transformer.layers[3].activation\n",
      ".decoder_transformer.transformer.layers[3].layer_scale_1.scale\n",
      ".decoder_transformer.transformer.layers[3].layer_scale_1.channel_last\n",
      ".decoder_transformer.transformer.layers[3].layer_scale_2.scale\n",
      ".decoder_transformer.transformer.layers[3].layer_scale_2.channel_last\n",
      ".decoder_transformer.transformer.layers[3].weights_per_step\n",
      ".decoder_transformer.transformer.layers[4].linear1.weight\n",
      ".decoder_transformer.transformer.layers[4].linear2.weight\n",
      ".decoder_transformer.transformer.layers[4].self_attn.embed_dim\n",
      ".decoder_transformer.transformer.layers[4].self_attn.causal\n",
      ".decoder_transformer.transformer.layers[4].self_attn.context\n",
      ".decoder_transformer.transformer.layers[4].self_attn.rope.max_period\n",
      ".decoder_transformer.transformer.layers[4].self_attn.num_heads\n",
      ".decoder_transformer.transformer.layers[4].self_attn.weights_per_step\n",
      ".decoder_transformer.transformer.layers[4].self_attn.in_proj.weight\n",
      ".decoder_transformer.transformer.layers[4].self_attn.out_proj.weight\n",
      ".decoder_transformer.transformer.layers[4].norm1.weight\n",
      ".decoder_transformer.transformer.layers[4].norm1.bias\n",
      ".decoder_transformer.transformer.layers[4].norm2.weight\n",
      ".decoder_transformer.transformer.layers[4].norm2.bias\n",
      ".decoder_transformer.transformer.layers[4].skip_self_attn\n",
      ".decoder_transformer.transformer.layers[4].activation\n",
      ".decoder_transformer.transformer.layers[4].layer_scale_1.scale\n",
      ".decoder_transformer.transformer.layers[4].layer_scale_1.channel_last\n",
      ".decoder_transformer.transformer.layers[4].layer_scale_2.scale\n",
      ".decoder_transformer.transformer.layers[4].layer_scale_2.channel_last\n",
      ".decoder_transformer.transformer.layers[4].weights_per_step\n",
      ".decoder_transformer.transformer.layers[5].linear1.weight\n",
      ".decoder_transformer.transformer.layers[5].linear2.weight\n",
      ".decoder_transformer.transformer.layers[5].self_attn.embed_dim\n",
      ".decoder_transformer.transformer.layers[5].self_attn.causal\n",
      ".decoder_transformer.transformer.layers[5].self_attn.context\n",
      ".decoder_transformer.transformer.layers[5].self_attn.rope.max_period\n",
      ".decoder_transformer.transformer.layers[5].self_attn.num_heads\n",
      ".decoder_transformer.transformer.layers[5].self_attn.weights_per_step\n",
      ".decoder_transformer.transformer.layers[5].self_attn.in_proj.weight\n",
      ".decoder_transformer.transformer.layers[5].self_attn.out_proj.weight\n",
      ".decoder_transformer.transformer.layers[5].norm1.weight\n",
      ".decoder_transformer.transformer.layers[5].norm1.bias\n",
      ".decoder_transformer.transformer.layers[5].norm2.weight\n",
      ".decoder_transformer.transformer.layers[5].norm2.bias\n",
      ".decoder_transformer.transformer.layers[5].skip_self_attn\n",
      ".decoder_transformer.transformer.layers[5].activation\n",
      ".decoder_transformer.transformer.layers[5].layer_scale_1.scale\n",
      ".decoder_transformer.transformer.layers[5].layer_scale_1.channel_last\n",
      ".decoder_transformer.transformer.layers[5].layer_scale_2.scale\n",
      ".decoder_transformer.transformer.layers[5].layer_scale_2.channel_last\n",
      ".decoder_transformer.transformer.layers[5].weights_per_step\n",
      ".decoder_transformer.transformer.layers[6].linear1.weight\n",
      ".decoder_transformer.transformer.layers[6].linear2.weight\n",
      ".decoder_transformer.transformer.layers[6].self_attn.embed_dim\n",
      ".decoder_transformer.transformer.layers[6].self_attn.causal\n",
      ".decoder_transformer.transformer.layers[6].self_attn.context\n",
      ".decoder_transformer.transformer.layers[6].self_attn.rope.max_period\n",
      ".decoder_transformer.transformer.layers[6].self_attn.num_heads\n",
      ".decoder_transformer.transformer.layers[6].self_attn.weights_per_step\n",
      ".decoder_transformer.transformer.layers[6].self_attn.in_proj.weight\n",
      ".decoder_transformer.transformer.layers[6].self_attn.out_proj.weight\n",
      ".decoder_transformer.transformer.layers[6].norm1.weight\n",
      ".decoder_transformer.transformer.layers[6].norm1.bias\n",
      ".decoder_transformer.transformer.layers[6].norm2.weight\n",
      ".decoder_transformer.transformer.layers[6].norm2.bias\n",
      ".decoder_transformer.transformer.layers[6].skip_self_attn\n",
      ".decoder_transformer.transformer.layers[6].activation\n",
      ".decoder_transformer.transformer.layers[6].layer_scale_1.scale\n",
      ".decoder_transformer.transformer.layers[6].layer_scale_1.channel_last\n",
      ".decoder_transformer.transformer.layers[6].layer_scale_2.scale\n",
      ".decoder_transformer.transformer.layers[6].layer_scale_2.channel_last\n",
      ".decoder_transformer.transformer.layers[6].weights_per_step\n",
      ".decoder_transformer.transformer.layers[7].linear1.weight\n",
      ".decoder_transformer.transformer.layers[7].linear2.weight\n",
      ".decoder_transformer.transformer.layers[7].self_attn.embed_dim\n",
      ".decoder_transformer.transformer.layers[7].self_attn.causal\n",
      ".decoder_transformer.transformer.layers[7].self_attn.context\n",
      ".decoder_transformer.transformer.layers[7].self_attn.rope.max_period\n",
      ".decoder_transformer.transformer.layers[7].self_attn.num_heads\n",
      ".decoder_transformer.transformer.layers[7].self_attn.weights_per_step\n",
      ".decoder_transformer.transformer.layers[7].self_attn.in_proj.weight\n",
      ".decoder_transformer.transformer.layers[7].self_attn.out_proj.weight\n",
      ".decoder_transformer.transformer.layers[7].norm1.weight\n",
      ".decoder_transformer.transformer.layers[7].norm1.bias\n",
      ".decoder_transformer.transformer.layers[7].norm2.weight\n",
      ".decoder_transformer.transformer.layers[7].norm2.bias\n",
      ".decoder_transformer.transformer.layers[7].skip_self_attn\n",
      ".decoder_transformer.transformer.layers[7].activation\n",
      ".decoder_transformer.transformer.layers[7].layer_scale_1.scale\n",
      ".decoder_transformer.transformer.layers[7].layer_scale_1.channel_last\n",
      ".decoder_transformer.transformer.layers[7].layer_scale_2.scale\n",
      ".decoder_transformer.transformer.layers[7].layer_scale_2.channel_last\n",
      ".decoder_transformer.transformer.layers[7].weights_per_step\n",
      ".decoder_transformer.input_dimension\n",
      ".decoder_transformer.output_dimensions[0]\n",
      ".decoder_transformer.conv_layout\n",
      ".encoder_transformer.transformer.positional_embedding\n",
      ".encoder_transformer.transformer.max_period\n",
      ".encoder_transformer.transformer.positional_scale\n",
      ".encoder_transformer.transformer.rope.max_period\n",
      ".encoder_transformer.transformer.layers[0].linear1.weight\n",
      ".encoder_transformer.transformer.layers[0].linear2.weight\n",
      ".encoder_transformer.transformer.layers[0].self_attn.embed_dim\n",
      ".encoder_transformer.transformer.layers[0].self_attn.causal\n",
      ".encoder_transformer.transformer.layers[0].self_attn.context\n",
      ".encoder_transformer.transformer.layers[0].self_attn.rope.max_period\n",
      ".encoder_transformer.transformer.layers[0].self_attn.num_heads\n",
      ".encoder_transformer.transformer.layers[0].self_attn.weights_per_step\n",
      ".encoder_transformer.transformer.layers[0].self_attn.in_proj.weight\n",
      ".encoder_transformer.transformer.layers[0].self_attn.out_proj.weight\n",
      ".encoder_transformer.transformer.layers[0].norm1.weight\n",
      ".encoder_transformer.transformer.layers[0].norm1.bias\n",
      ".encoder_transformer.transformer.layers[0].norm2.weight\n",
      ".encoder_transformer.transformer.layers[0].norm2.bias\n",
      ".encoder_transformer.transformer.layers[0].skip_self_attn\n",
      ".encoder_transformer.transformer.layers[0].activation\n",
      ".encoder_transformer.transformer.layers[0].layer_scale_1.scale\n",
      ".encoder_transformer.transformer.layers[0].layer_scale_1.channel_last\n",
      ".encoder_transformer.transformer.layers[0].layer_scale_2.scale\n",
      ".encoder_transformer.transformer.layers[0].layer_scale_2.channel_last\n",
      ".encoder_transformer.transformer.layers[0].weights_per_step\n",
      ".encoder_transformer.transformer.layers[1].linear1.weight\n",
      ".encoder_transformer.transformer.layers[1].linear2.weight\n",
      ".encoder_transformer.transformer.layers[1].self_attn.embed_dim\n",
      ".encoder_transformer.transformer.layers[1].self_attn.causal\n",
      ".encoder_transformer.transformer.layers[1].self_attn.context\n",
      ".encoder_transformer.transformer.layers[1].self_attn.rope.max_period\n",
      ".encoder_transformer.transformer.layers[1].self_attn.num_heads\n",
      ".encoder_transformer.transformer.layers[1].self_attn.weights_per_step\n",
      ".encoder_transformer.transformer.layers[1].self_attn.in_proj.weight\n",
      ".encoder_transformer.transformer.layers[1].self_attn.out_proj.weight\n",
      ".encoder_transformer.transformer.layers[1].norm1.weight\n",
      ".encoder_transformer.transformer.layers[1].norm1.bias\n",
      ".encoder_transformer.transformer.layers[1].norm2.weight\n",
      ".encoder_transformer.transformer.layers[1].norm2.bias\n",
      ".encoder_transformer.transformer.layers[1].skip_self_attn\n",
      ".encoder_transformer.transformer.layers[1].activation\n",
      ".encoder_transformer.transformer.layers[1].layer_scale_1.scale\n",
      ".encoder_transformer.transformer.layers[1].layer_scale_1.channel_last\n",
      ".encoder_transformer.transformer.layers[1].layer_scale_2.scale\n",
      ".encoder_transformer.transformer.layers[1].layer_scale_2.channel_last\n",
      ".encoder_transformer.transformer.layers[1].weights_per_step\n",
      ".encoder_transformer.transformer.layers[2].linear1.weight\n",
      ".encoder_transformer.transformer.layers[2].linear2.weight\n",
      ".encoder_transformer.transformer.layers[2].self_attn.embed_dim\n",
      ".encoder_transformer.transformer.layers[2].self_attn.causal\n",
      ".encoder_transformer.transformer.layers[2].self_attn.context\n",
      ".encoder_transformer.transformer.layers[2].self_attn.rope.max_period\n",
      ".encoder_transformer.transformer.layers[2].self_attn.num_heads\n",
      ".encoder_transformer.transformer.layers[2].self_attn.weights_per_step\n",
      ".encoder_transformer.transformer.layers[2].self_attn.in_proj.weight\n",
      ".encoder_transformer.transformer.layers[2].self_attn.out_proj.weight\n",
      ".encoder_transformer.transformer.layers[2].norm1.weight\n",
      ".encoder_transformer.transformer.layers[2].norm1.bias\n",
      ".encoder_transformer.transformer.layers[2].norm2.weight\n",
      ".encoder_transformer.transformer.layers[2].norm2.bias\n",
      ".encoder_transformer.transformer.layers[2].skip_self_attn\n",
      ".encoder_transformer.transformer.layers[2].activation\n",
      ".encoder_transformer.transformer.layers[2].layer_scale_1.scale\n",
      ".encoder_transformer.transformer.layers[2].layer_scale_1.channel_last\n",
      ".encoder_transformer.transformer.layers[2].layer_scale_2.scale\n",
      ".encoder_transformer.transformer.layers[2].layer_scale_2.channel_last\n",
      ".encoder_transformer.transformer.layers[2].weights_per_step\n",
      ".encoder_transformer.transformer.layers[3].linear1.weight\n",
      ".encoder_transformer.transformer.layers[3].linear2.weight\n",
      ".encoder_transformer.transformer.layers[3].self_attn.embed_dim\n",
      ".encoder_transformer.transformer.layers[3].self_attn.causal\n",
      ".encoder_transformer.transformer.layers[3].self_attn.context\n",
      ".encoder_transformer.transformer.layers[3].self_attn.rope.max_period\n",
      ".encoder_transformer.transformer.layers[3].self_attn.num_heads\n",
      ".encoder_transformer.transformer.layers[3].self_attn.weights_per_step\n",
      ".encoder_transformer.transformer.layers[3].self_attn.in_proj.weight\n",
      ".encoder_transformer.transformer.layers[3].self_attn.out_proj.weight\n",
      ".encoder_transformer.transformer.layers[3].norm1.weight\n",
      ".encoder_transformer.transformer.layers[3].norm1.bias\n",
      ".encoder_transformer.transformer.layers[3].norm2.weight\n",
      ".encoder_transformer.transformer.layers[3].norm2.bias\n",
      ".encoder_transformer.transformer.layers[3].skip_self_attn\n",
      ".encoder_transformer.transformer.layers[3].activation\n",
      ".encoder_transformer.transformer.layers[3].layer_scale_1.scale\n",
      ".encoder_transformer.transformer.layers[3].layer_scale_1.channel_last\n",
      ".encoder_transformer.transformer.layers[3].layer_scale_2.scale\n",
      ".encoder_transformer.transformer.layers[3].layer_scale_2.channel_last\n",
      ".encoder_transformer.transformer.layers[3].weights_per_step\n",
      ".encoder_transformer.transformer.layers[4].linear1.weight\n",
      ".encoder_transformer.transformer.layers[4].linear2.weight\n",
      ".encoder_transformer.transformer.layers[4].self_attn.embed_dim\n",
      ".encoder_transformer.transformer.layers[4].self_attn.causal\n",
      ".encoder_transformer.transformer.layers[4].self_attn.context\n",
      ".encoder_transformer.transformer.layers[4].self_attn.rope.max_period\n",
      ".encoder_transformer.transformer.layers[4].self_attn.num_heads\n",
      ".encoder_transformer.transformer.layers[4].self_attn.weights_per_step\n",
      ".encoder_transformer.transformer.layers[4].self_attn.in_proj.weight\n",
      ".encoder_transformer.transformer.layers[4].self_attn.out_proj.weight\n",
      ".encoder_transformer.transformer.layers[4].norm1.weight\n",
      ".encoder_transformer.transformer.layers[4].norm1.bias\n",
      ".encoder_transformer.transformer.layers[4].norm2.weight\n",
      ".encoder_transformer.transformer.layers[4].norm2.bias\n",
      ".encoder_transformer.transformer.layers[4].skip_self_attn\n",
      ".encoder_transformer.transformer.layers[4].activation\n",
      ".encoder_transformer.transformer.layers[4].layer_scale_1.scale\n",
      ".encoder_transformer.transformer.layers[4].layer_scale_1.channel_last\n",
      ".encoder_transformer.transformer.layers[4].layer_scale_2.scale\n",
      ".encoder_transformer.transformer.layers[4].layer_scale_2.channel_last\n",
      ".encoder_transformer.transformer.layers[4].weights_per_step\n",
      ".encoder_transformer.transformer.layers[5].linear1.weight\n",
      ".encoder_transformer.transformer.layers[5].linear2.weight\n",
      ".encoder_transformer.transformer.layers[5].self_attn.embed_dim\n",
      ".encoder_transformer.transformer.layers[5].self_attn.causal\n",
      ".encoder_transformer.transformer.layers[5].self_attn.context\n",
      ".encoder_transformer.transformer.layers[5].self_attn.rope.max_period\n",
      ".encoder_transformer.transformer.layers[5].self_attn.num_heads\n",
      ".encoder_transformer.transformer.layers[5].self_attn.weights_per_step\n",
      ".encoder_transformer.transformer.layers[5].self_attn.in_proj.weight\n",
      ".encoder_transformer.transformer.layers[5].self_attn.out_proj.weight\n",
      ".encoder_transformer.transformer.layers[5].norm1.weight\n",
      ".encoder_transformer.transformer.layers[5].norm1.bias\n",
      ".encoder_transformer.transformer.layers[5].norm2.weight\n",
      ".encoder_transformer.transformer.layers[5].norm2.bias\n",
      ".encoder_transformer.transformer.layers[5].skip_self_attn\n",
      ".encoder_transformer.transformer.layers[5].activation\n",
      ".encoder_transformer.transformer.layers[5].layer_scale_1.scale\n",
      ".encoder_transformer.transformer.layers[5].layer_scale_1.channel_last\n",
      ".encoder_transformer.transformer.layers[5].layer_scale_2.scale\n",
      ".encoder_transformer.transformer.layers[5].layer_scale_2.channel_last\n",
      ".encoder_transformer.transformer.layers[5].weights_per_step\n",
      ".encoder_transformer.transformer.layers[6].linear1.weight\n",
      ".encoder_transformer.transformer.layers[6].linear2.weight\n",
      ".encoder_transformer.transformer.layers[6].self_attn.embed_dim\n",
      ".encoder_transformer.transformer.layers[6].self_attn.causal\n",
      ".encoder_transformer.transformer.layers[6].self_attn.context\n",
      ".encoder_transformer.transformer.layers[6].self_attn.rope.max_period\n",
      ".encoder_transformer.transformer.layers[6].self_attn.num_heads\n",
      ".encoder_transformer.transformer.layers[6].self_attn.weights_per_step\n",
      ".encoder_transformer.transformer.layers[6].self_attn.in_proj.weight\n",
      ".encoder_transformer.transformer.layers[6].self_attn.out_proj.weight\n",
      ".encoder_transformer.transformer.layers[6].norm1.weight\n",
      ".encoder_transformer.transformer.layers[6].norm1.bias\n",
      ".encoder_transformer.transformer.layers[6].norm2.weight\n",
      ".encoder_transformer.transformer.layers[6].norm2.bias\n",
      ".encoder_transformer.transformer.layers[6].skip_self_attn\n",
      ".encoder_transformer.transformer.layers[6].activation\n",
      ".encoder_transformer.transformer.layers[6].layer_scale_1.scale\n",
      ".encoder_transformer.transformer.layers[6].layer_scale_1.channel_last\n",
      ".encoder_transformer.transformer.layers[6].layer_scale_2.scale\n",
      ".encoder_transformer.transformer.layers[6].layer_scale_2.channel_last\n",
      ".encoder_transformer.transformer.layers[6].weights_per_step\n",
      ".encoder_transformer.transformer.layers[7].linear1.weight\n",
      ".encoder_transformer.transformer.layers[7].linear2.weight\n",
      ".encoder_transformer.transformer.layers[7].self_attn.embed_dim\n",
      ".encoder_transformer.transformer.layers[7].self_attn.causal\n",
      ".encoder_transformer.transformer.layers[7].self_attn.context\n",
      ".encoder_transformer.transformer.layers[7].self_attn.rope.max_period\n",
      ".encoder_transformer.transformer.layers[7].self_attn.num_heads\n",
      ".encoder_transformer.transformer.layers[7].self_attn.weights_per_step\n",
      ".encoder_transformer.transformer.layers[7].self_attn.in_proj.weight\n",
      ".encoder_transformer.transformer.layers[7].self_attn.out_proj.weight\n",
      ".encoder_transformer.transformer.layers[7].norm1.weight\n",
      ".encoder_transformer.transformer.layers[7].norm1.bias\n",
      ".encoder_transformer.transformer.layers[7].norm2.weight\n",
      ".encoder_transformer.transformer.layers[7].norm2.bias\n",
      ".encoder_transformer.transformer.layers[7].skip_self_attn\n",
      ".encoder_transformer.transformer.layers[7].activation\n",
      ".encoder_transformer.transformer.layers[7].layer_scale_1.scale\n",
      ".encoder_transformer.transformer.layers[7].layer_scale_1.channel_last\n",
      ".encoder_transformer.transformer.layers[7].layer_scale_2.scale\n",
      ".encoder_transformer.transformer.layers[7].layer_scale_2.channel_last\n",
      ".encoder_transformer.transformer.layers[7].weights_per_step\n",
      ".encoder_transformer.input_dimension\n",
      ".encoder_transformer.output_dimensions[0]\n",
      ".encoder_transformer.conv_layout\n",
      ".quantizer.max_n_q\n",
      ".quantizer.n_q_semantic\n",
      ".quantizer.n_q_acoustic\n",
      ".quantizer.rvq_first.max_n_q\n",
      ".quantizer.rvq_first.n_q\n",
      ".quantizer.rvq_first.q_dropout\n",
      ".quantizer.rvq_first.dropout.p\n",
      ".quantizer.rvq_first.dropout.inference\n",
      ".quantizer.rvq_first.dimension\n",
      ".quantizer.rvq_first.input_dimension\n",
      ".quantizer.rvq_first.output_dimension\n",
      ".quantizer.rvq_first.bins\n",
      ".quantizer.rvq_first.decay\n",
      ".quantizer.rvq_first.input_proj.weight\n",
      ".quantizer.rvq_first.output_proj.weight\n",
      ".quantizer.rvq_first.vq.layers[0].epsilon\n",
      ".quantizer.rvq_first.vq.layers[0]._codebook.embedding\n",
      ".quantizer.rvq_first.vq.layers[0]._codebook.dim\n",
      ".quantizer.rvq_first.vq.layers[0]._codebook.codebook_size\n",
      ".quantizer.rvq_first.vq.layers[0]._codebook.decay\n",
      ".quantizer.rvq_first.vq.layers[0]._codebook.epsilon\n",
      ".quantizer.rvq_first.vq.layers[0]._codebook.threshold_usage_ratio\n",
      ".quantizer.rvq_first.vq.layers[0]._codebook.replaced_usage_ratio\n",
      ".quantizer.rvq_first.vq.layers[0]._codebook.check_unused_every\n",
      ".quantizer.rvq_first.vq.layers[0]._codebook._cached_initialized\n",
      ".quantizer.rvq_first.vq.layers[0]._codebook._next_unused_check\n",
      ".quantizer.rvq_first.vq.layers[0].codebook_size\n",
      ".quantizer.rvq_first.vq.codebook_offset\n",
      ".quantizer.rvq_rest.max_n_q\n",
      ".quantizer.rvq_rest.n_q\n",
      ".quantizer.rvq_rest.q_dropout\n",
      ".quantizer.rvq_rest.dropout.p\n",
      ".quantizer.rvq_rest.dropout.inference\n",
      ".quantizer.rvq_rest.dimension\n",
      ".quantizer.rvq_rest.input_dimension\n",
      ".quantizer.rvq_rest.output_dimension\n",
      ".quantizer.rvq_rest.bins\n",
      ".quantizer.rvq_rest.decay\n",
      ".quantizer.rvq_rest.input_proj.weight\n",
      ".quantizer.rvq_rest.output_proj.weight\n",
      ".quantizer.rvq_rest.vq.layers[0].epsilon\n",
      ".quantizer.rvq_rest.vq.layers[0]._codebook.embedding\n",
      ".quantizer.rvq_rest.vq.layers[0]._codebook.dim\n",
      ".quantizer.rvq_rest.vq.layers[0]._codebook.codebook_size\n",
      ".quantizer.rvq_rest.vq.layers[0]._codebook.decay\n",
      ".quantizer.rvq_rest.vq.layers[0]._codebook.epsilon\n",
      ".quantizer.rvq_rest.vq.layers[0]._codebook.threshold_usage_ratio\n",
      ".quantizer.rvq_rest.vq.layers[0]._codebook.replaced_usage_ratio\n",
      ".quantizer.rvq_rest.vq.layers[0]._codebook.check_unused_every\n",
      ".quantizer.rvq_rest.vq.layers[0]._codebook._cached_initialized\n",
      ".quantizer.rvq_rest.vq.layers[0]._codebook._next_unused_check\n",
      ".quantizer.rvq_rest.vq.layers[0].codebook_size\n",
      ".quantizer.rvq_rest.vq.layers[1].epsilon\n",
      ".quantizer.rvq_rest.vq.layers[1]._codebook.embedding\n",
      ".quantizer.rvq_rest.vq.layers[1]._codebook.dim\n",
      ".quantizer.rvq_rest.vq.layers[1]._codebook.codebook_size\n",
      ".quantizer.rvq_rest.vq.layers[1]._codebook.decay\n",
      ".quantizer.rvq_rest.vq.layers[1]._codebook.epsilon\n",
      ".quantizer.rvq_rest.vq.layers[1]._codebook.threshold_usage_ratio\n",
      ".quantizer.rvq_rest.vq.layers[1]._codebook.replaced_usage_ratio\n",
      ".quantizer.rvq_rest.vq.layers[1]._codebook.check_unused_every\n",
      ".quantizer.rvq_rest.vq.layers[1]._codebook._cached_initialized\n",
      ".quantizer.rvq_rest.vq.layers[1]._codebook._next_unused_check\n",
      ".quantizer.rvq_rest.vq.layers[1].codebook_size\n",
      ".quantizer.rvq_rest.vq.layers[2].epsilon\n",
      ".quantizer.rvq_rest.vq.layers[2]._codebook.embedding\n",
      ".quantizer.rvq_rest.vq.layers[2]._codebook.dim\n",
      ".quantizer.rvq_rest.vq.layers[2]._codebook.codebook_size\n",
      ".quantizer.rvq_rest.vq.layers[2]._codebook.decay\n",
      ".quantizer.rvq_rest.vq.layers[2]._codebook.epsilon\n",
      ".quantizer.rvq_rest.vq.layers[2]._codebook.threshold_usage_ratio\n",
      ".quantizer.rvq_rest.vq.layers[2]._codebook.replaced_usage_ratio\n",
      ".quantizer.rvq_rest.vq.layers[2]._codebook.check_unused_every\n",
      ".quantizer.rvq_rest.vq.layers[2]._codebook._cached_initialized\n",
      ".quantizer.rvq_rest.vq.layers[2]._codebook._next_unused_check\n",
      ".quantizer.rvq_rest.vq.layers[2].codebook_size\n",
      ".quantizer.rvq_rest.vq.layers[3].epsilon\n",
      ".quantizer.rvq_rest.vq.layers[3]._codebook.embedding\n",
      ".quantizer.rvq_rest.vq.layers[3]._codebook.dim\n",
      ".quantizer.rvq_rest.vq.layers[3]._codebook.codebook_size\n",
      ".quantizer.rvq_rest.vq.layers[3]._codebook.decay\n",
      ".quantizer.rvq_rest.vq.layers[3]._codebook.epsilon\n",
      ".quantizer.rvq_rest.vq.layers[3]._codebook.threshold_usage_ratio\n",
      ".quantizer.rvq_rest.vq.layers[3]._codebook.replaced_usage_ratio\n",
      ".quantizer.rvq_rest.vq.layers[3]._codebook.check_unused_every\n",
      ".quantizer.rvq_rest.vq.layers[3]._codebook._cached_initialized\n",
      ".quantizer.rvq_rest.vq.layers[3]._codebook._next_unused_check\n",
      ".quantizer.rvq_rest.vq.layers[3].codebook_size\n",
      ".quantizer.rvq_rest.vq.layers[4].epsilon\n",
      ".quantizer.rvq_rest.vq.layers[4]._codebook.embedding\n",
      ".quantizer.rvq_rest.vq.layers[4]._codebook.dim\n",
      ".quantizer.rvq_rest.vq.layers[4]._codebook.codebook_size\n",
      ".quantizer.rvq_rest.vq.layers[4]._codebook.decay\n",
      ".quantizer.rvq_rest.vq.layers[4]._codebook.epsilon\n",
      ".quantizer.rvq_rest.vq.layers[4]._codebook.threshold_usage_ratio\n",
      ".quantizer.rvq_rest.vq.layers[4]._codebook.replaced_usage_ratio\n",
      ".quantizer.rvq_rest.vq.layers[4]._codebook.check_unused_every\n",
      ".quantizer.rvq_rest.vq.layers[4]._codebook._cached_initialized\n",
      ".quantizer.rvq_rest.vq.layers[4]._codebook._next_unused_check\n",
      ".quantizer.rvq_rest.vq.layers[4].codebook_size\n",
      ".quantizer.rvq_rest.vq.layers[5].epsilon\n",
      ".quantizer.rvq_rest.vq.layers[5]._codebook.embedding\n",
      ".quantizer.rvq_rest.vq.layers[5]._codebook.dim\n",
      ".quantizer.rvq_rest.vq.layers[5]._codebook.codebook_size\n",
      ".quantizer.rvq_rest.vq.layers[5]._codebook.decay\n",
      ".quantizer.rvq_rest.vq.layers[5]._codebook.epsilon\n",
      ".quantizer.rvq_rest.vq.layers[5]._codebook.threshold_usage_ratio\n",
      ".quantizer.rvq_rest.vq.layers[5]._codebook.replaced_usage_ratio\n",
      ".quantizer.rvq_rest.vq.layers[5]._codebook.check_unused_every\n",
      ".quantizer.rvq_rest.vq.layers[5]._codebook._cached_initialized\n",
      ".quantizer.rvq_rest.vq.layers[5]._codebook._next_unused_check\n",
      ".quantizer.rvq_rest.vq.layers[5].codebook_size\n",
      ".quantizer.rvq_rest.vq.layers[6].epsilon\n",
      ".quantizer.rvq_rest.vq.layers[6]._codebook.embedding\n",
      ".quantizer.rvq_rest.vq.layers[6]._codebook.dim\n",
      ".quantizer.rvq_rest.vq.layers[6]._codebook.codebook_size\n",
      ".quantizer.rvq_rest.vq.layers[6]._codebook.decay\n",
      ".quantizer.rvq_rest.vq.layers[6]._codebook.epsilon\n",
      ".quantizer.rvq_rest.vq.layers[6]._codebook.threshold_usage_ratio\n",
      ".quantizer.rvq_rest.vq.layers[6]._codebook.replaced_usage_ratio\n",
      ".quantizer.rvq_rest.vq.layers[6]._codebook.check_unused_every\n",
      ".quantizer.rvq_rest.vq.layers[6]._codebook._cached_initialized\n",
      ".quantizer.rvq_rest.vq.layers[6]._codebook._next_unused_check\n",
      ".quantizer.rvq_rest.vq.layers[6].codebook_size\n",
      ".quantizer.rvq_rest.vq.codebook_offset\n",
      "._frame_rate\n",
      "._sample_rate\n",
      "._channels\n",
      ".encoder_frame_rate\n",
      ".downsample.learnt\n",
      ".downsample.channel_wise\n",
      ".downsample.conv.conv.conv.weight\n",
      ".downsample.conv.conv.norm_type\n",
      ".downsample.conv.causal\n",
      ".downsample.conv.pad_mode\n",
      ".upsample.learnt\n",
      ".upsample.channel_wise\n",
      ".upsample.convtr.convtr.convtr.weight\n",
      ".upsample.convtr.convtr.norm_type\n",
      ".upsample.convtr.causal\n",
      ".upsample.convtr.trim_right_ratio\n",
      ".dimension\n",
      ".resample_method\n"
     ]
    }
   ],
   "source": [
    "import jax.tree_util as jtu\n",
    "import jax.numpy as jnp\n",
    "\n",
    "their_params = {key: jax.numpy.array(numpy.array(value.detach())) for key, value in model.encoder_transformer.named_parameters()}\n",
    "\n",
    "def blop_trans(path, x):\n",
    "    path = jtu.keystr(path)[1:]\n",
    "    if \"encoder_transformer.transformer.layers[\" in path:\n",
    "        # print(path)\n",
    "        if \"weights_per_step\" in path:\n",
    "            return x\n",
    "        elif \"weight\" in path or \"bias\" in path or \".scale\" in path:\n",
    "            path = path.replace(\"encoder_transformer.\", \"\")\n",
    "            path = path.replace(\"in_proj.weight\", \"in_proj_weight\")\n",
    "            path = path.replace(\"[\", \".\").replace(\"]\", \"\")     \n",
    "            return their_params[path]\n",
    "    \n",
    "    return x\n",
    "\n",
    "jax_model = jtu.tree_map_with_path(blop_trans, jax_model)\n",
    "\n",
    "their_params = {key: jax.numpy.array(numpy.array(value.detach())) for key, value in model.decoder_transformer.named_parameters()}\n",
    "\n",
    "def blop_de_trans(path, x):\n",
    "    path = jtu.keystr(path)[1:]\n",
    "    # print(path)\n",
    "    if \"decoder_transformer.transformer.layers[\" in path:\n",
    "        if \"weights_per_step\" in path:\n",
    "            return x\n",
    "        elif \"weight\" in path or \"bias\" in path or \".scale\" in path:\n",
    "            # print(path)\n",
    "            path = path.replace(\"decoder_transformer.\", \"\")\n",
    "            path = path.replace(\"in_proj.weight\", \"in_proj_weight\")\n",
    "            path = path.replace(\"[\", \".\").replace(\"]\", \"\")     \n",
    "            return their_params[path]\n",
    "    print(path)\n",
    "    return x\n",
    "\n",
    "jax_model = jtu.tree_map_with_path(blop_de_trans, jax_model)\n",
    "\n",
    "their_params = {key: jax.numpy.array(numpy.array(value.detach())) for key, value in model.quantizer.named_parameters()}\n",
    "\n",
    "def quant_blop(path, x):\n",
    "    path = jtu.keystr(path)[1:]\n",
    "    if \"quantizer\" not in path:\n",
    "        return x\n",
    "    path = path.replace(\"quantizer.\", \"\")\n",
    "    if path in their_params.keys():\n",
    "        return their_params[path]\n",
    "    if path == \"rvq_first.vq.layers[0]._codebook.embedding\":\n",
    "        print(path)\n",
    "        return jnp.array(\n",
    "            numpy.array(model.quantizer.rvq_first.vq.layers[0]._codebook.embedding)\n",
    "        )\n",
    "    if \".vq.layers[\" in path and \"]._codebook.embedding\" in path:\n",
    "        idx =int( path.split(\"[\")[1][0])\n",
    "        print(path)\n",
    "        return jnp.array(\n",
    "            numpy.array(model.quantizer.rvq_rest.vq.layers[idx]._codebook.embedding)\n",
    "        )\n",
    "    return x\n",
    "\n",
    "\n",
    "jax_model = jtu.tree_map_with_path(quant_blop, jax_model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "their_params = {key: jax.numpy.array(numpy.array(value.detach())) for key, value in model.encoder.named_parameters()}\n",
    "\n",
    "mapping = {\n",
    "    \"first_layer.conv.conv.weight\": \"model.0.conv.conv.weight\",\n",
    "    \"first_layer.conv.conv.bias\": \"model.0.conv.conv.bias\",\n",
    "    \"last_layer.conv.conv.weight\": \"model.14.conv.conv.weight\",\n",
    "    \"last_layer.conv.conv.bias\": \"model.14.conv.conv.bias\",\n",
    "}\n",
    "\n",
    "def enc_blop(path, x):\n",
    "    path = jtu.keystr(path)\n",
    "    if \".encoder.\" not in path:\n",
    "        return x\n",
    "    path = path.replace(\".encoder.\", \"\")    # print(path)\n",
    "    if path in mapping.keys():\n",
    "        return their_params[mapping[path]] if \"weight\" in path else jnp.expand_dims(their_params[mapping[path]], -1)\n",
    "    if \"weight\" in path or \"bias\" in path:\n",
    "        if path[10:11] == \"1\":\n",
    "            idx = (int(path[7:8]) + 1) * 3\n",
    "\n",
    "            return their_params[f\"model.{idx}.conv.conv.weight\"] if \"weight\" in path else jnp.expand_dims(their_params[f\"model.{idx}.conv.conv.bias\"],-1)\n",
    "        else:\n",
    "            first_idx = 1 + (int(path[7:8])* 3)\n",
    "            second_idx = (int(path.split(\"blocks[\")[2][0]) * 2) + 1\n",
    "            return their_params[f\"model.{first_idx}.block.{second_idx}.conv.conv.weight\"] if \"weight\" in path else jnp.expand_dims(their_params[f\"model.{first_idx}.block.{second_idx}.conv.conv.bias\"], -1)\n",
    "    return x\n",
    "\n",
    "jax_model = jtu.tree_map_with_path(enc_blop, jax_model)\n",
    "\n",
    "their_params = {key: jax.numpy.array(numpy.array(value.detach())) for key, value in model.decoder.named_parameters()}\n",
    "\n",
    "mapping = {\n",
    "    \"first_layer.conv.conv.weight\": \"model.0.conv.conv.weight\",\n",
    "    \"first_layer.conv.conv.bias\": \"model.0.conv.conv.bias\",\n",
    "    \"last_layer.conv.conv.weight\": \"model.14.conv.conv.weight\",\n",
    "    \"last_layer.conv.conv.bias\": \"model.14.conv.conv.bias\",\n",
    "}\n",
    "\n",
    "def dec_blop(path, x):\n",
    "    path = jtu.keystr(path)\n",
    "    if \".decoder.\" not in path:\n",
    "        return x\n",
    "    print(path)\n",
    "    path = path.replace(\".decoder.\", \"\")    # print(path)\n",
    "    if path in mapping.keys():\n",
    "        print(path)\n",
    "        return their_params[mapping[path]] if \"weight\" in path else jnp.expand_dims(their_params[mapping[path]], -1)\n",
    "    if \"weight\" in path or \"bias\" in path:\n",
    "        if path[10:11] == \"0\":\n",
    "            idx = 2 + (int(path[7:8])* 3)\n",
    "            print(path)\n",
    "            rest = path.split(\"]\")[-1]\n",
    "            return jnp.swapaxes(jnp.flip(their_params[f\"model.{idx}{rest}\"], axis=2),axis1=0, axis2=1) if \"convtr.weight\" in path else their_params[f\"model.{idx}{rest}\"] if \"conv.weight\" in path else jnp.expand_dims(their_params[f\"model.{idx}.convtr.convtr.bias\"],-1)\n",
    "        else:\n",
    "            first_idx = 3 + (int(path[7:8])* 3)\n",
    "            second_idx = (int(path.split(\"blocks[\")[2][0]) * 2) + 1\n",
    "            print(path)\n",
    "            rest = path.split(\"]\")[-1]\n",
    "            \n",
    "            return jnp.swapaxes(jnp.flip(their_params[f\"model.{first_idx}.block.{second_idx}{rest}\"], axis=2), axis1=0, axis2=1) if \"convtr.weight\" in path else their_params[f\"model.{first_idx}.block.{second_idx}{rest}\"] if \"weight\" in path else jnp.expand_dims(their_params[f\"model.{first_idx}.block.{second_idx}{rest}\"], -1)\n",
    "    return x\n",
    "\n",
    "jax_model = jtu.tree_map_with_path(dec_blop, jax_model)\n",
    "\n",
    "their_params = {key: jax.numpy.array(numpy.array(value.detach())) for key, value in model.named_parameters()}\n",
    "\n",
    "\n",
    "def blop_model(path, x):\n",
    "    path = jtu.keystr(path)\n",
    "    print(path)\n",
    "    if \".upsample.convtr.convtr.convtr.weight\" == path:\n",
    "        return jnp.flip(their_params[\"upsample.convtr.convtr.convtr.weight\"], axis=2)\n",
    "    if \".downsample.conv.conv.conv.weight\" == path:\n",
    "        return their_params[\"downsample.conv.conv.conv.weight\"]\n",
    "    return x    \n",
    "\n",
    "    # return x÷\n",
    "\n",
    "jax_model = jtu.tree_map_with_path(blop_model, jax_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tugdual/moshi/moshi_jax/moshi_jax/quantization/core_vq.py:283: UserWarning: A JAX array is being set as static! This can result in unexpected behavior and is usually a mistake to do.\n",
      "  self._codebook = EuclideanCodebook(\n"
     ]
    }
   ],
   "source": [
    "import optax\n",
    "from tensorboardX import SummaryWriter\n",
    "from datasets import load_dataset\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import jax\n",
    "import jax.tree_util as jtu\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import librosa\n",
    "import equinox as eqx\n",
    "from hifigan import MultiScaleDiscriminator\n",
    "\n",
    "key1, key2 = jax.random.split(jax.random.key(1), 2)\n",
    "\n",
    "# model = jax_model\n",
    "\n",
    "model = get_jax_mimi(\"pouet\", jax.random.key(1))\n",
    "# def init_weights(path, x):\n",
    "#     global key1\n",
    "#     k, key1 = jax.random.split(key1)\n",
    "#     if \"weight\" in jtu.keystr(path):\n",
    "#         print(path)\n",
    "#         return jax.nn.initializers.xavier_uniform()(k, x.shape)\n",
    "#     elif \"bias\" in jtu.keystr(path):\n",
    "#         return jax.nn.initializers.constant(0)(k, x.shape)\n",
    "#     return x\n",
    "\n",
    "# model = jtu.tree_map_with_path(init_weights, model)\n",
    "\n",
    "optimizer = optax.chain(\n",
    "  optax.clip(1.0),\n",
    "  optax.adam(8e-4, 0.5, 0.9),\n",
    ")\n",
    "opt_state = optimizer.init(eqx.filter(model, eqx.is_array))\n",
    "\n",
    "scale_disc = MultiScaleDiscriminator(key=jax.random.key(1))\n",
    "scale_optimizer = optax.adam(1e-4)\n",
    "\n",
    "scale_opt_state = scale_optimizer.init(scale_disc)\n",
    "\n",
    "writer = SummaryWriter(log_dir='./runs/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "epochs = 50\n",
    "step = 0\n",
    "\n",
    "dataset = load_dataset(\"blabble-io/libritts_r\", \"clean\", streaming=True)\n",
    "\n",
    "def cut_up(samples):\n",
    "    list = []\n",
    "    for sample in samples[\"audio\"]:\n",
    "        resampled = librosa.resample(sample[\"array\"], orig_sr=22050, target_sr=16000)\n",
    "        for i in range(0, (int(len(resampled)//8000) -1)):\n",
    "            list.append(resampled[i*8000:i*8000+8000])\n",
    "    return {\"audio\": list}\n",
    "\n",
    "dataset = dataset.map(cut_up, batched=True, remove_columns=['text_normalized', 'text_original', 'speaker_id', 'path', 'chapter_id', 'id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from moshi_jax.moshi_jax.quantization.core_vq import EuclideanCodebook\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "dataloader= dataset[\"train.clean.360\"].batch(batch_size=batch_size)\n",
    "\n",
    "@eqx.filter_jit\n",
    "@eqx.filter_value_and_grad(has_aux=True)\n",
    "def calculate_losses(model, disc, x):\n",
    "    y, codes, _, _ = jax.vmap(model)(x)\n",
    "    # y = jnp.expand_dims(y, 1)\n",
    "    fake_pred, _ = jax.vmap(disc)(y) # At what point does disc think it's a true\n",
    "    G_loss = jnp.mean(jnp.abs(x-y)) * 30\n",
    "    # for fake in fake_pred:\n",
    "    #     G_loss += jax.numpy.mean((fake - 1) ** 2)\n",
    "    # for fake_map, real_map in zip (fake_feat, real_feat):\n",
    "    return G_loss, (y, codes)\n",
    "\n",
    "@eqx.filter_jit\n",
    "@eqx.filter_value_and_grad\n",
    "def calc_disc_loss(disc, x, y):\n",
    "    fake_pred, fake_feat = jax.vmap(disc)(y)\n",
    "    real_pred, real_feat = jax.vmap(disc)(x)\n",
    "    loss = 0\n",
    "    for fake_res, real_res in zip(fake_pred, real_pred):\n",
    "        fake_loss = jax.numpy.mean((fake_res) ** 2)\n",
    "        real_loss = jax.numpy.mean((real_res - 1) ** 2)\n",
    "        loss += fake_loss + real_loss\n",
    "        \n",
    "    return loss\n",
    "\n",
    "@eqx.filter_jit\n",
    "def update_codebook(model, codes: list, key: jax.Array=None):\n",
    "    for i in range(len(model.quantizer.rvq_first.vq.layers)):\n",
    "        value=EuclideanCodebook._check_expired_codes(model.quantizer.rvq_first.vq.layers[i]._codebook, codes[0][0][0], codes[0][1][0], key=key)\n",
    "        model = eqx.tree_at(lambda x: x.quantizer.rvq_first.vq.layers[i]._codebook, model, value)\n",
    "    for i in range(len(model.quantizer.rvq_rest.vq.layers)):\n",
    "        value = EuclideanCodebook._check_expired_codes(model.quantizer.rvq_rest.vq.layers[i]._codebook, codes[i+1][0][0], codes[i+1][1][0], key=key)\n",
    "        model = eqx.tree_at(lambda x: x.quantizer.rvq_rest.vq.layers[i]._codebook, model, value)\n",
    "        \n",
    "    return model\n",
    "\n",
    "@eqx.filter_jit\n",
    "def make_step(model, scale, optimizer, opt_state, scale_optimizer, scale_opt_state, x, key):\n",
    "    (total_loss, (y, codes)), grads = calculate_losses(model, scale, x)        \n",
    "    updates, opt_state = optimizer.update(grads, opt_state, eqx.filter(model, eqx.is_array))\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    # scale_loss, grads = calc_disc_loss(scale, x, y)\n",
    "    # updates, scale_opt_state = scale_optimizer.update(grads, scale_opt_state, scale)\n",
    "    # scale = eqx.apply_updates(scale, updates)\n",
    "    # model = update_codebook(model, codes, key)\n",
    "    \n",
    "    return (\n",
    "        model,\n",
    "        scale,\n",
    "        opt_state,\n",
    "        scale_opt_state,\n",
    "        total_loss,\n",
    "        0,\n",
    "        y\n",
    "    )\n",
    "    \n",
    "key = jax.random.key(3)\n",
    "    \n",
    "fig, ax = plt.subplots(1)\n",
    "ax.set_ylim(-1.0, 1.0)  # Adjust based on the expected range of your data\n",
    "\n",
    "# fig.show()\n",
    "step = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m k1, key \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(key)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mexpand_dims(jnp\u001b[38;5;241m.\u001b[39marray(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m]), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m model, scale_disc, opt_state, scale_opt_state, total_loss, scale_loss, y \u001b[38;5;241m=\u001b[39m \u001b[43mmake_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_disc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_opt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Log codebook updates to TensorBoard\u001b[39;00m\n\u001b[1;32m     11\u001b[0m step\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[4], line 45\u001b[0m, in \u001b[0;36mmake_step\u001b[0;34m(model, scale, optimizer, opt_state, scale_optimizer, scale_opt_state, x, key)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_step\u001b[39m(model, scale, optimizer, opt_state, scale_optimizer, scale_opt_state, x, key):\n\u001b[0;32m---> 45\u001b[0m     (total_loss, (y, codes)), grads \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_losses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m        \n\u001b[1;32m     46\u001b[0m     updates, opt_state \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mupdate(grads, opt_state, eqx\u001b[38;5;241m.\u001b[39mfilter(model, eqx\u001b[38;5;241m.\u001b[39mis_array))\n\u001b[1;32m     47\u001b[0m     model \u001b[38;5;241m=\u001b[39m eqx\u001b[38;5;241m.\u001b[39mapply_updates(model, updates)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/equinox/_ad.py:80\u001b[0m, in \u001b[0;36m_ValueAndGradWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m x, \u001b[38;5;241m*\u001b[39margs \u001b[38;5;241m=\u001b[39m args\n\u001b[1;32m     79\u001b[0m diff_x, nondiff_x \u001b[38;5;241m=\u001b[39m partition(x, is_inexact_array)\n\u001b[0;32m---> 80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfun_value_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiff_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnondiff_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/api.py:464\u001b[0m, in \u001b[0;36mvalue_and_grad.<locals>.value_and_grad_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    462\u001b[0m   ans, vjp_py \u001b[38;5;241m=\u001b[39m _vjp(f_partial, \u001b[38;5;241m*\u001b[39mdyn_args)\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 464\u001b[0m   ans, vjp_py, aux \u001b[38;5;241m=\u001b[39m \u001b[43m_vjp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m      \u001b[49m\u001b[43mf_partial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdyn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m _check_scalar(ans)\n\u001b[1;32m    467\u001b[0m tree_map(partial(_check_output_dtype_grad, holomorphic), ans)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/api.py:1966\u001b[0m, in \u001b[0;36m_vjp\u001b[0;34m(fun, has_aux, *primals)\u001b[0m\n\u001b[1;32m   1964\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1965\u001b[0m   flat_fun, out_aux_trees \u001b[38;5;241m=\u001b[39m flatten_fun_nokwargs2(fun, in_tree)\n\u001b[0;32m-> 1966\u001b[0m   out_primals, vjp, aux \u001b[38;5;241m=\u001b[39m \u001b[43mad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvjp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprimals_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1967\u001b[0m   out_tree, aux_tree \u001b[38;5;241m=\u001b[39m out_aux_trees()\n\u001b[1;32m   1968\u001b[0m out_primal_avals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(shaped_abstractify, out_primals)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/interpreters/ad.py:144\u001b[0m, in \u001b[0;36mvjp\u001b[0;34m(traceable, primals, has_aux)\u001b[0m\n\u001b[1;32m    142\u001b[0m   out_primals, pvals, jaxpr, consts \u001b[38;5;241m=\u001b[39m linearize(traceable, \u001b[38;5;241m*\u001b[39mprimals)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m   out_primals, pvals, jaxpr, consts, aux \u001b[38;5;241m=\u001b[39m \u001b[43mlinearize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraceable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprimals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munbound_vjp\u001b[39m(pvals, jaxpr, consts, \u001b[38;5;241m*\u001b[39mcts):\n\u001b[1;32m    147\u001b[0m   cts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(ct \u001b[38;5;28;01mfor\u001b[39;00m ct, pval \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(cts, pvals) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pval\u001b[38;5;241m.\u001b[39mis_known())\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/interpreters/ad.py:131\u001b[0m, in \u001b[0;36mlinearize\u001b[0;34m(traceable, *primals, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m _, in_tree \u001b[38;5;241m=\u001b[39m tree_flatten(((primals, primals), {}))\n\u001b[1;32m    130\u001b[0m jvpfun_flat, out_tree \u001b[38;5;241m=\u001b[39m flatten_fun(jvpfun, in_tree)\n\u001b[0;32m--> 131\u001b[0m jaxpr, out_pvals, consts \u001b[38;5;241m=\u001b[39m \u001b[43mpe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace_to_jaxpr_nounits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjvpfun_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_pvals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m out_primals_pvals, out_tangents_pvals \u001b[38;5;241m=\u001b[39m tree_unflatten(out_tree(), out_pvals)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(out_primal_pval\u001b[38;5;241m.\u001b[39mis_known() \u001b[38;5;28;01mfor\u001b[39;00m out_primal_pval \u001b[38;5;129;01min\u001b[39;00m out_primals_pvals)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/profiler.py:333\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    332\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/interpreters/partial_eval.py:681\u001b[0m, in \u001b[0;36mtrace_to_jaxpr_nounits\u001b[0;34m(fun, pvals, instantiate)\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m core\u001b[38;5;241m.\u001b[39mnew_main(JaxprTrace, name_stack\u001b[38;5;241m=\u001b[39mcurrent_name_stack) \u001b[38;5;28;01mas\u001b[39;00m main:\n\u001b[1;32m    680\u001b[0m   fun \u001b[38;5;241m=\u001b[39m trace_to_subjaxpr_nounits(fun, main, instantiate)\n\u001b[0;32m--> 681\u001b[0m   jaxpr, (out_pvals, consts, env) \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpvals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env\n\u001b[1;32m    683\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m main, fun, env\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/linear_util.py:188\u001b[0m, in \u001b[0;36mWrappedFun.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (gen, gen_static_args), out_store \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstores):\n\u001b[1;32m    187\u001b[0m   gen \u001b[38;5;241m=\u001b[39m gen(\u001b[38;5;241m*\u001b[39m(gen_static_args \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mtuple\u001b[39m(args)), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 188\u001b[0m   args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m   stack\u001b[38;5;241m.\u001b[39mappend((gen, out_store))\n\u001b[1;32m    190\u001b[0m gen \u001b[38;5;241m=\u001b[39m gen_static_args \u001b[38;5;241m=\u001b[39m out_store \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/interpreters/partial_eval.py:693\u001b[0m, in \u001b[0;36mtrace_to_subjaxpr_nounits\u001b[0;34m(main, instantiate, in_pvals)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;129m@lu\u001b[39m\u001b[38;5;241m.\u001b[39mtransformation\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrace_to_subjaxpr_nounits\u001b[39m(\n\u001b[1;32m    689\u001b[0m     main: core\u001b[38;5;241m.\u001b[39mMainTrace,\n\u001b[1;32m    690\u001b[0m     instantiate: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m Sequence[\u001b[38;5;28mbool\u001b[39m],\n\u001b[1;32m    691\u001b[0m     in_pvals: Sequence[PartialVal]):\n\u001b[1;32m    692\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(pv, PartialVal) \u001b[38;5;28;01mfor\u001b[39;00m pv \u001b[38;5;129;01min\u001b[39;00m in_pvals), in_pvals\n\u001b[0;32m--> 693\u001b[0m   out_tracers, jaxpr, out_consts, env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m _trace_to_subjaxpr_nounits(\n\u001b[1;32m    694\u001b[0m       main, instantiate, in_pvals)\n\u001b[1;32m    695\u001b[0m   out_pvals \u001b[38;5;241m=\u001b[39m [t\u001b[38;5;241m.\u001b[39mpval \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m out_tracers]\n\u001b[1;32m    696\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m out_tracers\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/interpreters/partial_eval.py:701\u001b[0m, in \u001b[0;36m_trace_to_subjaxpr_nounits\u001b[0;34m(main, instantiate, in_pvals)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_trace_to_subjaxpr_nounits\u001b[39m(main, instantiate, in_pvals):\n\u001b[1;32m    700\u001b[0m   trace \u001b[38;5;241m=\u001b[39m main\u001b[38;5;241m.\u001b[39mwith_cur_sublevel()\n\u001b[0;32m--> 701\u001b[0m   in_knowns  \u001b[38;5;241m=\u001b[39m [pval\u001b[38;5;241m.\u001b[39mis_known()     \u001b[38;5;28;01mfor\u001b[39;00m pval \u001b[38;5;129;01min\u001b[39;00m in_pvals]\n\u001b[1;32m    702\u001b[0m   in_consts  \u001b[38;5;241m=\u001b[39m [pval\u001b[38;5;241m.\u001b[39mget_known()    \u001b[38;5;28;01mfor\u001b[39;00m pval \u001b[38;5;129;01min\u001b[39;00m in_pvals \u001b[38;5;28;01mif\u001b[39;00m     pval\u001b[38;5;241m.\u001b[39mis_known()]\n\u001b[1;32m    703\u001b[0m   in_tracers \u001b[38;5;241m=\u001b[39m [trace\u001b[38;5;241m.\u001b[39mnew_arg(pval) \u001b[38;5;28;01mfor\u001b[39;00m pval \u001b[38;5;129;01min\u001b[39;00m in_pvals \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pval\u001b[38;5;241m.\u001b[39mis_known()]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/interpreters/partial_eval.py:701\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_trace_to_subjaxpr_nounits\u001b[39m(main, instantiate, in_pvals):\n\u001b[1;32m    700\u001b[0m   trace \u001b[38;5;241m=\u001b[39m main\u001b[38;5;241m.\u001b[39mwith_cur_sublevel()\n\u001b[0;32m--> 701\u001b[0m   in_knowns  \u001b[38;5;241m=\u001b[39m [\u001b[43mpval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_known\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m     \u001b[38;5;28;01mfor\u001b[39;00m pval \u001b[38;5;129;01min\u001b[39;00m in_pvals]\n\u001b[1;32m    702\u001b[0m   in_consts  \u001b[38;5;241m=\u001b[39m [pval\u001b[38;5;241m.\u001b[39mget_known()    \u001b[38;5;28;01mfor\u001b[39;00m pval \u001b[38;5;129;01min\u001b[39;00m in_pvals \u001b[38;5;28;01mif\u001b[39;00m     pval\u001b[38;5;241m.\u001b[39mis_known()]\n\u001b[1;32m    703\u001b[0m   in_tracers \u001b[38;5;241m=\u001b[39m [trace\u001b[38;5;241m.\u001b[39mnew_arg(pval) \u001b[38;5;28;01mfor\u001b[39;00m pval \u001b[38;5;129;01min\u001b[39;00m in_pvals \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pval\u001b[38;5;241m.\u001b[39mis_known()]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    # eqx.tree_serialise_leaves(f\"checkpoints/{epoch}.eqx\", model)\n",
    "\n",
    "    for batch in dataloader:\n",
    "        k1, key = jax.random.split(key)\n",
    "        input = jnp.expand_dims(jnp.array(batch[\"audio\"]), 1)\n",
    "        \n",
    "        model, scale_disc, opt_state, scale_opt_state, total_loss, scale_loss, y = make_step(model, scale_disc, optimizer, opt_state, scale_optimizer, scale_opt_state, input, k1)\n",
    "\n",
    "        # Log codebook updates to TensorBoard\n",
    "        step+=1\n",
    "        writer.add_scalar('Loss/FSQ', total_loss, step)\n",
    "        writer.add_scalar('Loss/ScaleDiscriminator', scale_loss, step)\n",
    "        if (step // batch_size) % 20 == 0:\n",
    "            ax.clear()\n",
    "            ax.plot(input[0][0])\n",
    "            ax.plot(y[0][0])\n",
    "            display(fig)\n",
    "            clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(model.quantizer.rvq_rest.vq.layers[4]._codebook.cluster_usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "coed = model.quantizer.rvq_first.vq.layers[0]._codebook\n",
    "\n",
    "replace_cluster_usage = (\n",
    "coed.replaced_usage_ratio * coed.cluster_usage / coed.codebook_size\n",
    ")\n",
    "\n",
    "print(replace_cluster_usage.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import librosa\n",
    "import jax.numpy as jnp\n",
    "y, sr = librosa.load(\"bria.mp3\")\n",
    "if sr != 24000:\n",
    "    y = librosa.resample(y, orig_sr=sr, target_sr=24000)\n",
    "\n",
    "y = jnp.array(y[None, None, :240000])\n",
    "y2 = torch.Tensor(numpy.array(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y2.dtype)\n",
    "\n",
    "out = jax.vmap(jax_model.encoder)(y)\n",
    "with torch.no_grad():\n",
    "    with model._context_for_encoder_decoder:\n",
    "        a = model.encoder(y2)\n",
    "# out2 = model.encoder(y2)\n",
    "print(out[0, 0, :10])\n",
    "print(a[0, 0, :10])\n",
    "\n",
    "b = model.encoder_transformer(a)\n",
    "out = jax.vmap(jax_model.encoder_transformer)(out)\n",
    "\n",
    "print(out[0][0, 0, :10])\n",
    "print(b[0][0, 0, :10])\n",
    "print(out[0].shape)\n",
    "c = model._to_framerate(b[0])\n",
    "out = (jax.vmap(jax_model._to_framerate)(out[0]), out[1:])\n",
    "\n",
    "print(out[0][0, 0, :10])\n",
    "print(c[0, 0, :10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# out = jax.numpy.array(out2[0].detach().numpy())\n",
    "\n",
    "d = model.quantizer(c, model.frame_rate)\n",
    "out = jax.vmap(jax_model.quantizer, in_axes=(0, None))(out[0], jax_model.frame_rate)\n",
    "# print(out2.x.shape)\n",
    "print(out[0].shape)\n",
    "e = model._to_encoder_framerate(d.x)\n",
    "out = (jax.vmap(jax_model._to_encoder_framerate)(out[0]), out[1:])\n",
    "# out = (jax.numpy.array(out2.x.detach().numpy()), out[1:])\n",
    "# print(out2.x.shape)\n",
    "print(out[0].shape)\n",
    "# out = jax.numpy.array(out2[0].detach().numpy())\n",
    "\n",
    "out = jax.vmap(jax_model.decoder_transformer)(out[0])\n",
    "e = model.decoder_transformer(e)\n",
    "# print(\"AAA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# out2 = torch.from_numpy(numpy.array(out[0]))\n",
    "# out = jax.numpy.array(out2[0].detach().numpy())\n",
    "# print(out2[0].shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "    with model._context_for_encoder_decoder:\n",
    "        f = model.decoder(e[0])\n",
    "out = jax.vmap(jax_model.decoder)(out[0])\n",
    "\n",
    "# print(out[0, 0, :10])\n",
    "# print(out2[0, 0, :10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "print(y2.shape)\n",
    "# IPython.display.Audio(y2[0], rate=24000)\n",
    "IPython.display.Audio(out[0][0], rate=24000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1,)\n",
    "ax.plot(y[0][0][:24000])\n",
    "ax.plot(out[0][0][:24000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "codebook_size = 512\n",
    "dim = 5\n",
    "\n",
    "x = jax.random.normal(key=jax.random.key(1), shape=(10, 5))\n",
    "flat_codes = jax.numpy.arange(0, 10, dtype=\"int64\")\n",
    "flat_codes = flat_codes.at[2].set(1)\n",
    "print(flat_codes)\n",
    "their_x = torch.from_numpy(numpy.array(x))\n",
    "their_flat_codes = torch.from_numpy(numpy.array(flat_codes))\n",
    "index = einops.repeat(their_flat_codes, \"n -> n d\", d=dim).to(torch.int64)\n",
    "embedding_sum = torch.zeros((codebook_size, dim))\n",
    "\n",
    "\n",
    "print(index)\n",
    "print(their_x)\n",
    "\n",
    "embedding_sum2 = jax.numpy.zeros((codebook_size, dim))\n",
    "print(embedding_sum.scatter_add_(0, index, their_x))\n",
    "print(embedding_sum2.at[flat_codes].add(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xtts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
